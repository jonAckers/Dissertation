\chapter{Implementation}
\label{chap:implementation}


\section{System Architecture}
\indent \indent
This section is dedicated to detailing the high-level architecture and design of the project. It will discuss the purpose of each component and the software engineering principles applied to make design choices.

\subsection{Software Design}
\indent \indent
The project's design began with understanding the MLaaS threat model described in ยง\ref{sec:threatModel}. Figure \ref{fig:abstractNetwork} depicts an abstract layout of the project's core components. There are two categories of components: \textit{online} describing inference performed directly in response to a user's request to emulate the MLaaS model, and \textit{offline} describing inference results generated on batches of data, independent of the frontend. The offline components are used during the implementation and evaluation stages to develop and refine the system (labelled \textit{testing}), and to collect and analyse the results presented in Chapter \ref{chap:evaluation} (labelled \textit{evaluation}).
\smallskip \\ \indent
An overview of the project's repository is given in Figure \ref{fig:filetree}. Excluding SEAL, all code was written for the project. An object-oriented design methodology was used to allow separate components to be implemented independently and isolate the layers depicted in Figure \ref{fig:abstraction}. Another advantage is that it allows straightforward substitution of different inference methods and encryption schemes.
\begin{figure}[ht]
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \scalebox{0.3}{\input{figures/abstractNetwork}}
        \captionsetup{justification=centering}
        \caption[Project Components]{The project components.\medskip\\Online:\sethlcolor{ckksRed} \hl{\quad\quad\quad\quad} \quad Offline:\sethlcolor{mekksBlue} \hl{\quad\quad\quad\quad}}
        \label{fig:abstractNetwork}
    \end{subfigure}%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \scalebox{0.53}{\input{figures/abstractInference}}
        \caption{Abstract layers of the inference stack.}
        \label{fig:abstractInference}
    \end{subfigure}%
    \caption{High-level implementation overview.}
    \label{fig:abstraction}
\end{figure}
\smallskip \\ \indent
The application's online components can be split into four abstract layers, from high-level interface to low-level implementation.
\begin{enumerate}
    \item The \textit{graphical user interface} component allows users to configure the encryption scheme and inference method, and upload videos.
    \item The \textit{client} and \textit{server} components are responsible for managing the network communication to allow videos to be transferred to the server for inference, and the results returned to the client.
    \item The \textit{encryption} component provides an API for the cryptographic primitives to allow videos to be encrypted and decrypted in the client, and operated on by the server in the \textit{inference} component.
    \item The implementation of the cryptographic primitives provided by the CKKS scheme through the \textit{MeKKS} and \textit{SEAL-Python} libraries.
\end{enumerate}
\indent
Figure \ref{fig:abstractInference} provides insight into the composition of the inference component. The scope of this project only considers the layers above encryption primitives. However, lower layers exist. Another layer relevant to this investigation may be the hardware implementation. The IoT devices used for surveillance cause significant constraints on computational performance. Accelerators, such as GPUs, could be investigated to improve running times of cryptographic operations on these devices ~\cite{Badawi}.
\begin{figure}[h!]
    \includegraphics[scale=0.68]{figures/repositoryOverview}
    \caption[Repository Overview]{Repository overview. In total, 6,322 lines of code were written.}
    \label{fig:filetree}
\end{figure}
\subsection{Class Structure}
\indent \indent
This section provides a more thorough insight into the project's structure. Figure \ref{fig:clientUML} details the arrangement of the client-side, Figure \ref{fig:serverUML} contains the classes composing the server-side, and Figure \ref{fig:mekksUML} depicts the structure of the MeKKS library. While overlaps between diagrams exist, they have been separated into three figures for clarity.
\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.44\textwidth}
        \includegraphics[scale=0.13]{figures/clientClasses}
        \captionsetup{justification=centering}
        \caption{Client\medskip\\External CKKS classes:\sethlcolor{ckksRed} \hl{\quad\quad\quad\quad}\smallskip\\External MeKKS classes:\sethlcolor{mekksBlue} \hl{\quad\quad\quad\quad}}
        \label{fig:clientUML}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.55\textwidth}
        \centering
        \includegraphics[scale=0.15]{figures/serverClasses}
        \captionsetup{justification=centering}
        \caption{Server\medskip\\External CKKS classes:\sethlcolor{ckksRed} \hl{\quad\quad\quad\quad}\smallskip\\Abstract HE objects:\sethlcolor{homomorphicGreen} \hl{\quad\quad\quad\quad}}
        \label{fig:serverUML}
    \end{subfigure}
    \caption[UML Class Diagrams for Core Components]{UML Class Diagrams showing the core project components.}
\end{figure}
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.2]{figures/mekksClasses}
    \captionsetup{justification=centering}
    % \scalebox{0.6}{\input{figures/mekksUML}}
    \caption[MeKKS UML Class Diagram]{UML Class Diagram showing the components of the MeKKS encryption scheme.}
    \label{fig:mekksUML}
\end{figure}






\section{Encryption Schemes}
\indent \indent
This section briefly discusses the fundamentals of the CKKS HE scheme described by Cheon et al.\ \cite{CKKS}, and the reimplementation completed for this project. Also, to provide an investigation into specialising HE schemes, a bespoke CKKS implementation, called MeKKS, is detailed.
\subsection{CKKS Primitives}
\label{sec:CKKS}
\indent \indent
Fundamentally, the CKKS scheme encrypts plaintext polynomials into ciphertext polynomials. Addition and multiplication operations can then be performed on the data, introducing uncertainty. Ciphertext polynomials can be decrypted to retrieve approximations of plaintext polynomials, with the accuracy depending on the number of operations performed.
\smallskip \\ \indent
To encrypt vectors of real values, they must first be encoded as polynomials in the ring $\mathcal{R} = \mathbb{Z}[X] / (X^N + 1)$, where $N$ is a power of 2. During encoding, the real values are rounded, so precision is preserved by multiplying by a \textit{scaling factor}, $\Delta$. Once the vector has been encoded, it can be encrypted into a \textit{pair} of polynomials: the ciphertext.
\smallskip \\ \indent
Consider two vectors $\vec{v}$ and $\vec{w}$. Before encoding, the vectors are scaled to $\Delta \vec{v}$ and $\Delta \vec{w}$.  They can then be encrypted and multiplied to produce ciphertext equivalents of $\Delta^2 (\vec{v} \bigodot \vec{w})$. This implies that a sequence of multiplications will continuously increase the scale factor. To overcome this, CKKS introduces a \textit{rescaling} procedure, which can be understood as dividing the ciphertext by $\Delta$.
\smallskip \\ \indent
However, rescaling cannot be repeatedly applied to allow unlimited multiplications. CKKS is a levelled HE scheme, so each ciphertext resides at a discrete level. Each level has a coefficient modulus $q$ that dictates a ciphertext's coefficients must be in $\mathbb{Z}_q$. When first encrypted, a polynomial exists in the maximum level, $L$, with coefficient modulus $Q = q_0 \dot \Delta^L$, for a \textit{base modulus} $q_0$. When a ciphertext is rescaled, the coefficient modulus is divided by $\Delta$, reducing it to $q_0 \cdot \Delta^{L-1}$. Hence, the ciphertext is lowered to the next level. If a ciphertext reaches level 0, no further multiplications can be applied to preserve the encrypted values. To ensure decryption produces correct results, a polynomial's coefficients cannot exceed $q_0$. Therefore, practical implementations will use a $q_0$ much larger than $\Delta$.

\subsubsection{Encoding and Decoding}
\indent \indent
The plaintext message space is the \textit{cyclotomic polynomial ring} $\mathcal{R} = \mathbb{Z}[X] / \Phi_M(X)$,  for the $M$-th cyclotomic polynomial, where $M$ is a power of two. Encoding describes mapping a complex vector to an element in $\mathcal{R}$, and decoding is the process of reversing it.
\smallskip \\ \indent
Another way of defining decoding is as a mapping of an element $r \in \mathcal{R}$ to a vector in $\mathbb{C}^N$ using the embedding $\sigma : \frac{\mathbb{R}[X]}{X^N + 1} \rightarrow \mathbb{C}^N$. This is applied to each root of $\Phi_M(X)$ through the evaluation of $r$ as
\begin{equation}
    \sigma(r) = (r(\xi), r(\xi^3), \ldots, r(\xi^{M-1})) = (r(\xi^k) \: | \: k \in \mathbb{Z}^*_N) \in \mathbb{C}^N
\end{equation}
where $\xi = e^{\frac{2 \pi i}{M}}$ is a primitive $M$-th root of unity.
\smallskip \\ \indent
However, encoding requires the input to be \textit{scaled} and \textit{rounded}, and decoding requires half of the complex vector to be \textit{discarded} to produce a one-to-one mapping.
\smallskip \\ \indent
Hence, the CKKS encoding function is given by Equation \ref{eq:encode}.
\begin{equation}
    \label{eq:encode}
    \texttt{Encode}(\vec{v}, \Delta) : \mathbb{C}^\frac{N}{2} \times \mathbb{Z}^+ \rightarrow \mathcal{R}
\end{equation}
\subsubsection{Operations}
\indent \indent
Figure \ref{fig:ckksOps} lists the operations supported by CKKS and their definitions. An important observation is that ciphertext multiplication requires polynomials to be multiplied. This makes multiplication a much more computationally expensive operation than addition.
\begin{figure}[h!]
    \centering
    \resizebox{0.7\textwidth}{!}{%
    \begin{tcolorbox}
        \begin{itemize}[leftmargin=0.1cm]
            \item \texttt{KeyGen} : sample $s \leftarrow \chi_{\text{key}}$, $\vec{r}' \rightarrow \mathcal{R}_{qL}$, $\vec{r}' \rightarrow \mathcal{R}_{P \cdot qL}$, $\vec{e} \leftarrow \chi_{\text{err}}$, and $\vec{e}' \leftarrow \chi_{\text{err}}$.
            \begin{enumerate}
                \item Calculate $\vec{a} := - \vec{r} \vec{s} + \vec{e} \mod q_L$ and $\vec{a}' := -\vec{r}' \vec{s} + \vec{e}' + P \vec{s}^2 \mod P \cdot qL$.
                \item Return $\texttt{SecretKey} := (1, \vec{s})$, $\texttt{PublicKey} := (\vec{a}, \vec{r})$, $\texttt{EvaluationKey} := (\vec{a}', \vec{r}')$.
            \end{enumerate}
            \item $\texttt{Enc}_\texttt{PublicKey}(\vec{m})$ : sample $\vec{v} \leftarrow \chi_{\text{enc}}$ and $\vec{e}_0, \vec{e}_1 \leftarrow \chi_{\text{err}}$.
            \begin{enumerate}
                \item Calculate $\vec{c}_0 := \vec{v} \cdot \vec{a} + \vec{m} + \vec{e}_0$ and $\vec{c}_1 := \vec{v} \cdot \vec{r} + \vec{e}_1$.
                \item Return $(\vec{c}_0, \vec{c}_1) \mod q_L$.
            \end{enumerate}
            \item $\texttt{Dec}_\texttt{SecretKey}(\vec{c}_0 + \vec{c}_1 \cdot \vec{s}) = (\vec{c}_0 + \vec{c}_1) \mod q_l$.
            \item $\texttt{Add}((\vec{c}_0, \vec{c}_1), (\vec{d}_0, \vec{d}_1)) = (\vec{c}_0 + \vec{d}_0, \vec{c}_1 + \vec{d}_1) \mod q_l$.
            \item $\texttt{AddPlain}((\vec{c}_0, \vec{c}_1), x) = (c_0 + \texttt{Enc}_\texttt{PublicKey}(x), \vec{c}_1) \mod q_l$.
            \item $\texttt{Multiply}((\vec{c}_0, \vec{c}_1), (\vec{d}_0, \vec{d}_1)) = (\vec{r}_0, \vec{r}_1) + (\lfloor P^{-1} \cdot \vec{r}_2 \cdot \texttt{EvaluationKey}_0 \rceil, \lfloor P^{-1} \cdot \vec{r}_2 \cdot \texttt{EvaluationKey}_1  \rceil) \mod q_l$.
            \item $\texttt{MultiplyPlain}((\vec{c}_0, \vec{c}_1), x) = (\vec{c}_0 \cdot \texttt{Enc}_\texttt{PublicKey}(x), \vec{c}_1 \cdot \texttt{Enc}_\texttt{PublicKey}(x)) \mod q_l$.
        \end{itemize}
    \end{tcolorbox}}
    \caption[CKKS Operations]{A list of the basic operations supported by the CKKS scheme. $P$ is a large scaling factor and $\lfloor \cdot \rceil$ denotes rounding to the nearest integer. Recreated from \cite{CKKS}.}
    \label{fig:ckksOps}
\end{figure}
\subsubsection{Rescaling}
\indent \indent
The rescaling function introduced above is defined by equation \ref{eq:rescale}.
\begin{equation}
    \label{eq:rescale}
    \texttt{Rescale}(\vec{c}, \Delta^{l_\text{new}}) : \mathcal{R}^2_{ql} \times \mathbb{Z}^+ \rightarrow \mathcal{R}^2_{ql} = \lfloor \Delta^{l_\text{new} - l} \cdot \vec{c} \rceil \mod q_{l_\text{new}}
\end{equation}
\indent
Rescaling truncates some of the least-significant bits of a ciphertext by dividing by a power of the scaling factor. Practically, this is performed after every multiplication to scale $\Delta^2$ down to $\Delta$.
\subsubsection{Rotations}
\indent \indent
An advantage of RLWE based HE schemes over others is the ability to \textit{rotate} ciphertext slots. Given a vector $\vec{V} = [v_0, v_1, v_2, \ldots, v_n]$ and an offset $d$. A rotation would cyclically shift each element by $d$ indices\footnote{For example, $\vec{v} = [1, 2, 3, 4, 5]$ and $d = 2$ would produce $\vec{v}' = [4, 5, 1, 2, 3]$.}. This is achieved by performing a \textit{Galois automorphism} on the ciphertext using standard results from \textit{Galois theory}. However, these operations are the most expensive operations offered by RLWE-based schemes  ~\cite{RotationBad}.
\subsubsection{RNS Optimisations}
\indent \indent
SEAL utilises a \textit{residue number system} (RNS) to overcome the slow computations caused by very large polynomial coefficients requiring arbitrary precision arithmetic [RNS]. The Chinese Remainder Theorem (CRT) is exploited to decompose the coefficients in $\mathbb{Z}_q$ into $n$ smaller ones: $\mathbb{Z}_{p_1}, \ldots, \mathbb{Z}_{p_n}$. To do this, the CRT uses a \textit{moduli switching chain} $p_1, \ldots, p_n$ such that $\prod_i p_i = q$ and each $p_i$ is a prime number requiring fewer than 64-bits to store. Hardware limitations mean it is faster to compute the $n$ separate multiplications in $p_1, \ldots, p_n$ than it is to compute a single multiplication in $q$. The results of the separate multiplications can be combined using the isomorphism between $\mathbb{Z}_q$ and $\mathbb{Z}_{p_1}, \ldots, \mathbb{Z}_{p_n}$.
\smallskip \\ \indent
Unfortunately, as a result of the RNS optimisation, $q_l = q_0 \Delta^l$ cannot always be maintained due to the difficulty of finding every $q_l$ as the product of $n$ primes. To overcome this, SEAL instead defines $q_l = q_0 \prod_{i=1}^l p_i$ where $q_0$ must be prime. Where the original CKKS scheme divides by $\Delta$ to rescale, SEAL's implementation divides by $p_l$. Consequently, the scaling factor is not exactly equal to $\Delta$. In practice, the scaling factor must be manually reset to $\Delta$ after each rescaling.
\subsection{Exploiting Specialisation}
\label{sec:mekks}
\indent \indent
As an extension to the project, the CKKS scheme was reimplemented from first principles to create MeKKS. This allowed the investigation of a HE scheme specialised for this application through simplification of data structures and removal of unnecessary functionality.
\subsubsection{Limitations}
\label{sec:mekksLimitations}
\indent \indent
The primary limitation of MeKKS was the language used for implementation. Traditionally, encryption schemes are written in C or C++ to expose low-level functionality. Consequently, the more computationally expensive operations, such as large prime number multiplications, would run much quicker than in Python.
\smallskip \\ \indent
Also, in contrast to CKKS, MeKKS is severely limited by optimisations that have been applied. This extension began by focussing on improving HE understanding, so the foundations of CKKS were implemented. However, CKKS has been significantly optimised since it was originally proposed. For example, using residue number systems, bootstrapping extensions, and reduced approximation error ~\cite{RNS, BootstrappingHEAAN, RAE}. The project's time constraints meant that defining a clear implementation endpoint was essential for scheduling. Therefore, MeKKS is significantly less optimised than SEAL's CKKS, limiting performance expectations.
\subsubsection{Extending MeKKS}
\indent \indent
The first iteration of the MeKKS implementation began with the scheme presented by Cheon et al.\ in 2016 ~\cite{CKKS}. This allowed all of the functionality described in ยง\ref{sec:CKKS} to be implemented following the SEAL API, enabling easy integration into the core project.
\smallskip \\ \indent
However, this implementation's performance meant it was infeasible for recording results. Therefore, the bootstrapping procedure from the next HEAAN iteration was implemented ~\cite{BootstrappingHEAAN}. Bootstrapping takes advantage of the \textit{approximate computation} characteristic of HEAAN to evaluate the decryption formula approximately so a ciphertext can be obtained in a large modulus. Hence, an approximation of the \textit{modular reduction} formula was implemented to enable efficient evaluation using standard operations - where the error induced by the approximation is small enough to maintain precision.
\smallskip \\ \indent
Using the observation that the modular reduction function, $F(t) = [t]_q$, is the identity near zero and periodic in $q$, bootstrapping uses a trigonometric function to approximate the function when $t = \langle \texttt{Ciphertext}, \texttt{SecretKey} \rangle$ is close to a multiple of $q$ (the ciphertext modulus). Specifically, using the \textit{sine} function given by Equation \ref{eq:bootstrapping}.
\begin{equation}
    \label{eq:bootstrapping}
    [\langle \texttt{Ciphertext}, \texttt{SecretKey} \rangle]_q = \frac{q}{2 \pi} \cdot \sin \left( \frac{2 \pi}{q} \cdot \langle \texttt{Ciphertext}, \texttt{SecretKey} \rangle \right) + O(\epsilon^3 \cdot q)
\end{equation}
when $F(\langle \texttt{Ciphertext}, \texttt{SecretKey} \rangle) < \epsilon \cdot q$.
\smallskip \\ \indent
The Taylor polynomial can be used to approximate the trigonometric function to enable evaluation in the HE domain. The input, $t$, is bounded by $K \cdot q$ for some constant $K = O(\lambda)$, where $\lambda$ is the security parameter. The degree of the Taylor polynomial should be at least $O(K \cdot q)$ to ensure the error term is small enough on the interval $(-K \cdot q, K \cdot q)$. Cheon et al.\ proposed using the \textit{Paterson-Stockmeyer} method to reduce the calculation complexity ~\cite{BootstrappingHEAAN, Paterson}. However, the complexity of recryption grows exponentially with the depth of the decryption circuit, which still has a substantial impact.
\smallskip \\ \indent
Figure \ref{fig:bootstrapping} provides a graphical representation of the bootstrapping approximation.
\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.25]{figures/bootstrapping.png}
    \caption[Bootstrapping Procedure]{Modular Reduction and Scaled Sine Functions. Reproduced from \cite{BootstrappingHEAAN}.}
    \label{fig:bootstrapping}
\end{figure}
\subsubsection{Specialising MeKKS}
\indent \indent
The principal optimisations of MeKKS came through data representation. When integrating SEAL, a Python wrapper for a C++ implementation was used. Consequently, the application was forced to handle large C++ objects, abstracted through the wrapper. Python struggled to manipulate these objects efficiently, specifically when serialising data for transmission. However, since MeKKS was written in Python, object attributes and functionality were exposed, providing much more efficient manipulation.
\smallskip \\ \indent
Another optimisation came through the specialisation of the library. The project schedule ensured MeKKS was only implemented once the application's core had been complete. Therefore, the required functionality had been finalised, so only necessary operations were implemented for MeKKS. This reduced the size of objects and removed any unused computation, making execution more efficient. For example, the most expensive operations offered by CKKS are rotations. However, since the application does not use them, they were not included in MeKKS.




\section{Optimising Network Throughput}
\label{sec:networking}
\indent \indent
This section details the techniques investigated for optimising communication between the client and server. An established limitation of HE is the size of ciphertexts ~\cite{Makkaoui}. Consequently, the \textit{transmission time} of data is significantly impaired. The \textit{transmission time} can be defined by Equation \ref{eq:transmission}.
\begin{equation}
    \text{transmission time} = \frac{\text{video size }\textit{(bytes)}}{\text{transmission rate }\textit{(bytes/second)}}
    \label{eq:transmission}
\end{equation}
\indent \indent
Transferring large volumes of data to the cloud is a critical component of the MLaaS model, so a substantial portion of the investigation was dedicated to reducing transmission time. The problem was considered from two angles: reducing the \textit{video size} and increasing the \textit{transmission rate}.
\subsection{Reducing Video Size}
\subsubsection{Seam Carving}
\label{sec:seamCarving}
\indent \indent
Developed by Avidan and Shamir in 2007, \textit{seam carving} describes a method of resizing images using \textit{geometric constraints} while also considering \textit{image content} ~\cite{SeamCarving}. Consequently, an image can be resized while preserving important features, such as people or buildings. There are two categories of techniques for distinguishing these features. \textit{Top-down} methods use tools such as \textit{face detectors} to highlight features in the image ~\cite{Viola}. \textit{Bottom-up} approaches use saliency maps\footnote{a representation highlighting the regions of an image where a person's eyes are first drawn ~\cite{Saliency}.} to locate important areas ~\cite{Itti}.
\smallskip \\ \indent
The details of the original seam carving paper, and a depiction of the algorithm, are included in Appendix \ref{app:seamCarving}. However, a more advanced algorithm was implemented to investigate more optimal results.
\smallskip \\ \indent
To quantify the importance of a pixel, seam carving defines an \textit{energy function}. Rubinstein et al.\ \cite{Rubinstein} proposed the \textit{forward energy} function using dynamic programming. This method calculates the energy of a pixel by accounting for the impact on future energies if it is removed. To achieve this, the \textit{energy difference} function is defined by Equation \ref{eq:energyDiff}. The cost of removing pixels, $C$, is measured as the forward differences between the pixels that would become neighbours after deletion. There are three cases for this: diagonally adjacent in each direction and orthogonally adjacent, depicted by Figure \ref{fig:adjacency} and defined by Equation \ref{eq:adjacency}.
\begin{equation}
    \label{eq:energyDiff}
    \Delta E_{i+1} = E(\vec{I}_{i+1}) - (E(\vec{I}_i) - E(C_i))
\end{equation}
\begin{figure}
    \scalebox{0.5}{\input{figures/forwardEnergy}}
    \caption[Potential Seam Costs]{There are three possible vertical seam costs for pixel $(i, j)$. After removing the seam, new neighbours (blue) and new edges (red) are created.}
    \label{fig:adjacency}
\end{figure}
\begin{align}
    \label{eq:adjacency}
    \begin{split}
        C_L(i, j) &= |\vec{I}(i, j+1) - \vec{I}(i, j-1)| + |\vec{I}(i-1, j) - \vec{I}(i, j-1)|
    \end{split} \\
    \begin{split}
        C_U(i, j) &= |\vec{I}(i, j+1) - \vec{I}(i, j-1)|
    \end{split} \\
    \begin{split}
        C_R(i, j) &= |\vec{I}(i, j+1) - \vec{I}(i, j-1)| + |\vec{I}(i-1, j) - \vec{I}(i, j+1)|
    \end{split}
\end{align}
\indent
Once the energy has been calculated, the image can be split into \textit{seams}. A \textit{vertical seam} is a path of pixels connecting the top of an image to the bottom, only including a single pixel in each row. Likewise, a \textit{horizontal seam} connects the left of an image to the right, only including a single pixel from each column. These is defined by Equation \ref{eq:vSeam} and Equation \ref{eq:hSeam} respectively.
\begin{subequations}
    \begin{equation}
        \label{eq:vSeam}
        \vec{s}^\vec{x} = \{s^x_i\}^n_{i=1} = \{(x(i), i) \: s.t. \: \forall i \: |x(i) - x(i-1)| \leq 1\}^n_{i=1}
    \end{equation}
    \begin{equation}
        \label{eq:hSeam}
        \vec{s}^\vec{y} = \{s^y_j\}^m_{j=1} = \{(j, y(j)) \: s.t. \: \forall j \: |y(j) - y(j-1)| \leq 1\}^m_{j=1}
    \end{equation}
\end{subequations}
for an $n \times m$ image, \vec{I}, where $x : [1, \ldots, n] \rightarrow [1, \ldots, m]$ and $y : [1, \ldots, m] \rightarrow [1, \ldots, n]$.
\smallskip \\ \indent
From this, the \textit{optimal seam} can be found. That is, the seam that minimises the \textit{seam cost} - the sum of all pixel costs in the path. Some implementations will use variants of Dijkstra's algorithm for this. Alternatively, Equation \ref{eq:forwardEnergy} defines a dynamic programming approach.
\begin{equation}
    \label{eq:forwardEnergy}
    M(i, j) = e(i, j) + \min 
    \begin{cases}
        M(i-1, j-1) + C_L(i,j) \\
        M(i-1, j) + C_U(i,j) \\
        M(i-1, j+1) + C_R(i,j)
    \end{cases}
\end{equation}
\indent
It is important to note that there have been several extensions to seam carving that may apply to this project. Particularly, optimisations for videos by introducing two-dimensional seams to allow time to be accounted for, and implementations using GPUs to reduce execution time ~\cite{Rubinstein, Duarte}.

\subsubsection{Graph Representation}
\label{sec:graphReps}
\indent \indent
Representing images using graphs has several advantages. Firstly, graphs are discrete, mathematically simple objects with an established set of provably correct algorithms. More pertinently, graphs provide flexible representations that can be used to tune image size.
\smallskip \\ \indent
Graph-based image processing methods operate on \textit{pixel adjacency graphs} - graphs whose vertex set is the set of image pixels and edge set defines adjacency of pixels. An example of some pixel adjacency graphs is given by Figure \ref{fig:pixelAdjacency}. Three-dimensional pixel adjacency graphs can account for relationships between video frames. Examples of are depicted by Figure \ref{fig:3dAdjacency}.
\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.29\textwidth}
        \centering
        \captionsetup{justification=centering}
        \scalebox{0.5}{\input{figures/adjacency1}}
        \caption{A 2D image with $4 \times 4$ pixels.}
    \end{subfigure} \hfill%
    \begin{subfigure}[b]{0.29\textwidth}
        \centering
        \captionsetup{justification=centering}
        \scalebox{0.5}{\input{figures/adjacency2}}
        \caption{A 4-connected pixel adjacency graph.}
    \end{subfigure} \hfill%
    \begin{subfigure}[b]{0.29\textwidth}
        \centering
        \captionsetup{justification=centering}
        \scalebox{0.5}{\input{figures/adjacency3}}
        \caption{An 8-connected pixel adjacency graph.}
    \end{subfigure}%
    \caption[Pixel Adjacency Graphs]{Pixel adjacency graphs.}
    \label{fig:pixelAdjacency}
\end{figure}
\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.29\textwidth}
        \centering
        \captionsetup{justification=centering}
        \scalebox{0.55}{\input{figures/3dAdjacency1}}
        \caption{A video with three frames of $3 \times 3$ pixels.}
    \end{subfigure} \hfill%
    \begin{subfigure}[b]{0.29\textwidth}
        \centering
        \captionsetup{justification=centering}
        \scalebox{0.25}{\input{figures/3dAdjacency2}}
        \caption{A 6-connected 3D pixel adjacency graph.}
    \end{subfigure} \hfill%
    \begin{subfigure}[b]{0.29\textwidth}
        \centering
        \captionsetup{justification=centering}
        \scalebox{0.25}{\input{figures/3dAdjacency3}}
        \caption{An 18-connected 3D pixel adjacency graph.}
    \end{subfigure}%
    \caption[3D Pixel Adjacency Graphs]{3D pixel adjacency graphs.}
    \label{fig:3dAdjacency}
\end{figure}
\smallskip \\ \indent
However, to improve the video size, pixel adjacency graphs must be extended to \textit{region adjacency graphs}. Rather than representing each pixel with a node, pixels are amalgamated into regions represented by a single node. Figure \ref{fig:pixelToRegion} provides a pictorial example of this.
\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \captionsetup{justification=centering}
        \scalebox{0.35}{\input{figures/pixelToRegion1}}
        \caption{A pixel adjacency graph.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \captionsetup{justification=centering}
        \scalebox{0.35}{\input{figures/pixelToRegion2}}
        \caption{The corresponding region adjacency graph.}
    \end{subfigure}
    \caption[Pixel to Region Conversion]{Converting a pixel adjacency graph to a region adjacency graph.}
    \label{fig:pixelToRegion}
\end{figure}
\smallskip \\ \indent
To achieve this, similarity between pixels must be quantified. This constitutes another image segmentation problem. Consequently, established algorithms producing valid solutions exist. Unsupervised clustering algorithms such as \textit{the watershed transform} \cite{Watershed} or \textit{k-means clustering} \cite{KMeans} are two methods that have proved useful in existing works.
\smallskip \\ \indent
Importantly for this investigation, the number of regions in the image will directly impact the transmission time. Reducing the number of nodes in the graph is advantageous because it reduces the amount of data transmitted. However, this also reduces image resolution. Consequently, removing too many nodes from the graph will remove clarity, making inference worthless. Figure \ref{fig:regions} depicts this. Therefore, a balance must be struck heuristically to maximise video size reduction while minimising impact on video quality. This optimal point is likely to be different for every image, adding a further layer of complexity.
\begin{figure}[h!]
    \centering
    \begin{subfigure}[t]{0.29\textwidth}
        \centering
        \captionsetup{justification=centering}
        \includegraphics[scale=0.25]{figures/adjacencyBefore}
        \caption{A photograph of a cup of coffee.}
    \end{subfigure} \hfill%
    \begin{subfigure}[t]{0.29\textwidth}
        \centering
        \captionsetup{justification=centering}
        \includegraphics[scale=0.25]{figures/adjacencyMiddle}
        \caption{The picture of coffee split into approximately 400 regions.}
    \end{subfigure} \hfill%
    \begin{subfigure}[t]{0.29\textwidth}
        \centering
        \captionsetup{justification=centering}
        \includegraphics[scale=0.25]{figures/adjacencyAfter}
        \caption{The picture of coffee split into approximately 10 regions.}
    \end{subfigure}%
    \caption[Tuning Region Adjacency Graphs]{A demonstration of the impact of reducing the number of regions on image quality. Images taken from \cite{Foveal}.}
    \label{fig:regions}
\end{figure}
\smallskip \\ \indent
Using similar techniques to seam carving, it is possible to limit the severity of this trade-off. For example, \textit{Foveal sampling}, depicted in Figure \ref{fig:foveal}, utilises the visual activity of the eye to determine regions ~\cite{Foveal}. The \textit{Fovea centralis} is a region of the retina responsible for the sharp central vision used by mammals to focus on particular objects. Consequently, its shape can be used to bias the placement of nodes of a graph to prioritise more critical areas. This allows the region budget to be used more efficiently to reduce noticeable quality reduction. Other techniques have been developed using saliency maps or similar.
\begin{figure}[h!]
    \centering
    \begin{subfigure}[t]{0.29\textwidth}
        \centering
        \captionsetup{justification=centering}
        \includegraphics[scale=0.25]{figures/fovealBefore}
        \caption{A photograph of some ducks.}
    \end{subfigure} \hfill%
    \begin{subfigure}[t]{0.29\textwidth}
        \centering
        \captionsetup{justification=centering}
        \includegraphics[scale=0.2]{figures/fovealMap}
        \caption{The retinal topography of a kangaroo.}
    \end{subfigure} \hfill%
    \begin{subfigure}[t]{0.29\textwidth}
        \centering
        \captionsetup{justification=centering}
        \includegraphics[scale=0.2]{figures/fovealResult}
        \caption{The photograph of ducks sampled using the retinal topography of a kangaroo.}
    \end{subfigure}%
    \caption[Foveal Sampling]{An example of Foveal Sampling. Images taken from \cite{Foveal}.}
    \label{fig:foveal}
\end{figure}
\subsection{Increasing Transmission Rate}
\label{sec:parallelisation}
\indent \indent
This section targets the bottlenecks limiting the transmission rate of the system by investigating the application of \textit{parallel computing}.
\smallskip \\ \indent
Figure \ref{fig:parallelStack} depicts the abstract layers of the application's networking processes. The components are split into two categories: the \textit{client} and \textit{server} components handling communication, and the \textit{packing} and \textit{unpacking} components manipulating data before and after transmission. Parallelisation was selected for investigation because of the growing support in computer architecture. Traditionally, computer design has focussed on \textit{sequential computing} to improve performance. However, factors such as Dennard scaling \cite{Dennard} mean the improvements predicted by Moore's law \cite{Moore} may not continue indefinitely. Therefore, architects are utilising multiprocessing to achieve similar gains. Consequently, this seemed like a viable opportunity for the investigation to consider future iterations of surveillance technology.
\begin{figure}[h!]
    \centering
    \scalebox{0.4}{\input{figures/parallelStack}}
    \captionsetup{justification=centering}
    \caption[Abstract view of parallel processes]{The stages required for a video to be sent across the network.}
    \label{fig:parallelStack}
\end{figure}
\subsubsection{Communication}
\label{sec:communication}
\indent \indent
Parallelisation already exists in networking protocols. The \textit{Transmission Control Protocol} uses a \textit{sliding window protocol} to send groups of data packets concurrently, ensuring they are reordered correctly when received. More information is provided in Appendix \ref{app:slidingWindow}. These protocols exist in the \textit{data-link} layer of the \textit{OSI network model} ~\cite{OSI}. This section of the investigation aimed to evaluate the result of moving parallelisation higher up this abstract stack.
\smallskip \\ \indent
Taking inspiration from sliding windows, instead of using a single network socket to send data in a single stream, videos are split into frames, and each frame is divided into packets. Meanwhile, a pool of threads is created, analogous to the window, and mapped to packets, enabling multiple sockets to be opened in parallel, increasing the amount of data sent simultaneously.
\smallskip \\ \indent
However, there are limitations to this technique. Firstly, more data must be transmitted than during sequential communication. The algorithm is non-deterministic, so the order of packet delivery cannot be guaranteed. Consequently, further information must be attached to packets to ensure videos are reassembled correctly. While this is worth noting, the size of this additional data is negligible compared to HE data, so it is not a critical issue.
\smallskip \\ \indent
A more pressing concern is the overhead of creating threads and establishing connections. This cost means creating too many threads will countermand parallelisation benefits and make transmission slower. Therefore, an optimal balance between the cost of parallelisation and the volume of data to send must be achieved to maximise improvements.
\subsubsection{Data Manipulation}
\indent \indent
Splitting videos into small packets has further advantages when preparing data for transmission. Before data can be transmitted, it must be prepared, or \textit{packed}. When data arrives at its destination, it must be \textit{unpacked}. There are three distinct stages to packing: \textit{encryption}, \textit{compression}, and \textit{serialisation}, which are reversed during unpacking. The encryption stage is missing from the server-side pipeline.
\smallskip \\ \indent
In a na\"ive implementation, each video pixel might be packed individually. However, this can be improved. The CKKS scheme operates on vectors of real values. Therefore, decomposing a frame into rows provides the opportunity for \textit{vectorising} the application by encrypting each row as a single ciphertext object. Consequently, the number of ciphertexts needed is reduced - for an $n \times m$ pixel frame, the number of objects is reduced from $nm$ to $n$ - so the time and space complexity of encryption is reduced from quadratic to linear complexity.
\smallskip \\ \indent
Moreover, decomposing a video into independent packets allows the packing and unpacking pipelines to be executed in parallel. Therefore, the cost of encryption, compression, and serialisation can be amortised by preparing multiple packets concurrently. This process can take advantage of the multithreading described in ยง\ref{sec:communication} so that no further overhead of thread creation is required. Consequently, potential improvement is only limited by the need for each stage to terminate entirely before the next begins, so reducing the size of packets allows pipelines to terminate faster. However, smaller quanta require more communication threads, so this cannot be indefinitely exploited.





\section{Moving Object Detection Inference Algorithms}
\label{sec:inference}
\indent \indent
This section discusses the implementation of the moving object detection algorithms. It will examine the necessary modifications to support HE video data in ยง\ref{sec:adaptations}, and detail the more complex algorithms needed for unsupervised learning in ยง\ref{sec:OMM} and ยง\ref{sec:EMAlg}.
\subsection{Adapting Inference Algorithms for Homomorphic Encryption}
\label{sec:adaptations}
\indent \indent
There were two main challenges to overcome when implementing inference algorithms for HE. Firstly, the number of operations that can be applied is limited by the depth of the ciphertext. Secondly, the set of operations supported by HE is more limited than is available for plain data. Consequently, compromises were made to produce accurate results without introducing detrimental side effects - for example, to accommodate more operations, the depth of a ciphertext could be increased, but this significantly increases the size of ciphertexts, making data transmission infeasible.
\smallskip \\ \indent
The adaptations implemented for the less complex algorithms are detailed below. The techniques investigated for implementing GMMs are evaluated in ยง\ref{sec:integration}.
\subsubsection{Frame Differencing}
\indent \indent
Frame differencing required few modifications to accommodate HE data. Only a single operation is required: \textit{subtraction}, and is provided by the standard CKKS implementation. Moreover, subtraction does not impact a ciphertext's level, so does not require size modifications. As well as making networking more efficient, this makes the inference algorithm faster because the data being operated over is minimised.
\smallskip \\ \indent
Therefore, the only modification required to support HE data is to replace the subtraction function in the traditional algorithm with the subtraction circuit of a HE library.
\subsubsection{Mean Filter}
\indent \indent
A na\"ive mean filter implementation recalculates the reference frame repeatedly by storing a list of all observed frames and computing their mean each when a new frame is received. Alternatively, a more advanced implementation will use an iterative function to update the mean, removing the requirement to store preceding frames.
\smallskip \\ \indent
Evidently, the second method will produce better time and space complexity when considering plain video data. However, when investigating HE data, the distinction is not as clear. The second method becomes problematic in that the mean must have both \textit{multiplication} and \textit{addition} operations applied when updating. Each time a multiplication circuit is applied, the ciphertext will be reduced. Therefore, the ciphertext must have the same number of levels as frames in the video. This quickly becomes infeasible; increasing ciphertext size so far would severely increase the time complexity of operations and make transmission times impractically slow. Consequently, this method can be immediately ruled out.
\smallskip \\ \indent
The first method encounters different difficulties. One issue is the space complexity of storing all frames in the video. Since HE data is very large, storing multiple copies of each frame significantly increases memory usage. Similarly, HE operations are noticeably slower than plaintext operations. Therefore, while the cost of recalculating the mean of plaintext data may be negligible, HE implementations will become increasingly slower. However, a solution can be derived. A compromise can be achieved by limiting the number of frames stored, forgetting the oldest frame whenever a new one arrives. Importantly, this may reduce inference accuracy, so a balance must be struck between running time and inference quality.
\subsubsection{Gaussian Average}
\indent \indent
Similarly to a mean filter, a plaintext Gaussian average implementation might fit distributions using iterative formulae for the mean and variance. Copied from ยง\ref{sec:gaussianAverage} for convenience, Equation \ref{eq:mean2} and Equation \ref{eq:variance2} provide these formulae. Consequently, the plaintext implementation will be infeasible in the HE domain. Fortunately, adaptations can be made.
\begin{equation}
    \label{eq:mean2}
    \mu_t =
    \begin{cases}
        f_0 & \text{if $t = 0$} \\
        \alpha f_t + (1 - \alpha) \mu_{t-1} & \text{otherwise}
    \end{cases}
\end{equation}
\begin{equation}
    \label{eq:variance2}
    \sigma^2_t =
    \begin{cases}
        c & \text{if $t = 0$} \\
        d^2 \alpha + (1 - \alpha) \sigma^2_{t-1} & \text{otherwise}
    \end{cases}
\end{equation}
where $\alpha$ determines the temporal window, $d$ represents the Euclidean distance between a pixel and the mean, and $c$ is some constant.
\smallskip \\ \indent
Firstly, expanding the iterative formulae highlights that the frames of a video are \textit{multiplicatively independent}, as demonstrated by Equation \ref{eq:expansion} for the fifth frame of a video.
\begin{equation}
    \label{eq:expansion}
    \begin{split}
        \mu_4 &= \alpha f_4 + \alpha (1-\alpha) f_3 + \alpha (1-\alpha)^2 f_2 + \alpha (1-\alpha)^3 f_1 + (1-\alpha)^4 f_0 \\ 
        \sigma^2_4 &= \alpha d^2_4 + \alpha (1-\alpha) d^2_3  + \alpha (1-\alpha)^2 d^2_2 + \alpha (1-\alpha)^3 d^2_1 + (1-\alpha)^4 c
    \end{split}
\end{equation}
\indent
Helpfully, the coefficient terms across calculations are identical, so computation can be shared. The server decides the value of $\alpha$ before runtime, so coefficients can be pre-calculated in the clear before being encoded for HE multiplication. This means that only a single multiplication needs to be applied to each frame. Then, results can be summed to give the mean and variance. Consequently, the number of levels required for a ciphertext is significantly reduced, so time and space complexity are also reduced.
\smallskip \\ \indent
However, this method still requires storing a list of preceding frames to enable recalculating terms. Like with mean filter, this introduces a trade-off between running time and memory usage against inference accuracy.
\subsection{Online Mixture Model}
\label{sec:OMM}
\indent \indent
In 1999 Stauffer and Grimson proposed \textit{adaptive background mixture models} for real-time moving object detection ~\cite{Stauffer}. To overcome a single Gaussian distribution's inability to model natural pixel variation, they proposed a mixture of adaptive Gaussians. The advantage of their technique over other GMMs is that the model runs \textit{online}: the model can be fitted, and results returned in a single phase. While this is useful for real-time inference acting on a constant data stream, it has the disadvantage of producing less accurate results earlier in the execution sequence.
\subsubsection{Fitting}
\indent \indent
For a particular pixel, the values that it records are known as the \textit{pixel process}. This is a time series of pixel values such that, at any time $t$, the process of pixel $(x,y)$ is defined by Equation \ref{eq:pixelProcess}.
\begin{equation}
    \label{eq:pixelProcess}
    \{X_0, \ldots, X_t\} = \{I(x,y,i) \; | \; 0 \leq i \leq t\}
\end{equation}
where $I$ is the image sequence.
\smallskip \\ \indent
The history of a pixel can be modelled as a mixture of $K$ Gaussian distributions. $K$ is usually between 3 and 5, depending on available memory and computational power availability. Given a pixel process, the probability of observing the pixel value at time $t$ is given by
\begin{equation}
    \probP (X_t) = \sum^K_{i=1} \omega_{i,t} \times \eta(X_t, \mu_{i,t}, \Sigma_{i,t})
\end{equation}
where $\omega_{i,t}$ represents an estimate of the proportion of the data accounted for by the $i^{\text{th}}$ Gaussian at time $t$, $\mu_{i,t}$ and $\Sigma_{i,t}$ are the mean and covariance matrix of the $i^{\text{th}}$ Gaussian at time $t$ respectively. $\eta$ is the Gaussian probability density function given by Equation \ref{eq:pdf}.
\begin{equation}
    \label{eq:pdf}
    \eta (X, \mu, \Sigma) = \frac{1}{(2\pi)^\frac{n}{2} |\Sigma|^\frac{1}{2}} e^{-\frac{1}{2} (X - \mu)^T \Sigma^{-1} (X - \mu)}
\end{equation}
\indent
To maximise the likelihood of the observed data, a \textit{k-means} approximation was selected to engender an online implementation. Each pixel in a new frame is compared against the existing Gaussian distributions until a \textit{match} is found. A match occurs when a pixel value is within a predefined number of standard deviations of a distribution. The number of standard deviations will vary across distributions as each distribution will account for different factors such as lighter or shadier regions. 
\smallskip \\ \indent
If none of the distributions match a pixel's value, the least likely Gaussian is replaced by a new distribution defined with the pixel as its mean, an initially high variance, and low prior weight. Then, the prior weights are adjusted at time $t$ using Equation \ref{eq:priors}.
\begin{equation}
    \label{eq:priors}
    \omega_{k,t} = (1-\alpha)\omega_{k,t-1} + \alpha M_{k,t}
\end{equation}
where $\alpha$ is a learning rate, and $M$ is an indicator function of $1$ if Gaussian $k$ at time $t$ matched, and $0$ otherwise. After this approximation is complete, the weights are normalised.
\smallskip \\ \indent
For unmatched distributions, $\mu$ and $\sigma$ are unchanged. The parameters of the matching distributions are updated according to Equation \ref{eq:muAndSigma}, where $rho$ is defined by Equation \ref{eq:rho}.
\begin{subequations}
    \label{eq:muAndSigma}
    \begin{equation}
        \mu_t = (1 - \rho) \mu_{t-1} + \rho X_t
    \end{equation}
    \begin{equation}
        \Sigma^2_t = (1 - \rho) \Sigma^2_{t-1} + \rho(X_t - \mu_t)^T(X_t - \mu_t)
    \end{equation}
\end{subequations}
\begin{equation}
    \label{eq:rho}
    \rho = \alpha \eta(X_t, \mu_k, \Sigma_k)
\end{equation}
\subsubsection{Predicting}
\indent \indent
Once the parameters have been updated, each pixel is labelled by determining the Gaussian component most likely produced by the background process. This decision assumes that there will be little variance in the Gaussians when the frame is static and a large variance when a new object occludes the background. Consequently, a function defining the proportion of the GMM representing the background process is required.
\smallskip \\ \indent
First, the Gaussians are ordered by the value of $\frac{\omega}{\Sigma}$. The definitions of $\omega$ and $\Sigma$ mean that this value will increase as the distribution gains more evidence and the variance decreases. This value will only differ from the last iteration for matching distributions, so the sorting process can be made more efficient. The ordered list can then be iterated over, and the first $B$ distributions are taken as the \textit{background model}, where 
\begin{equation}
    \label{eq:gmmInequality}
    B = argmin_b \left( \sum^b_{k=1} \omega_k > T \right)
\end{equation}
The threshold, $T$, is a measure of how much data should be accounted for. In other words, the best-fitted distributions are taken until a certain portion of recent data has been considered. 
\smallskip \\ \indent
Once the background model has been decided, it can be used to label the pixel as either \textit{foreground} or \textit{background}, allowing the moving objects to be extracted as foreground.
\subsection{Expectation-Maximisation Algorithm}
\label{sec:EMAlg}
\indent \indent
Proposed by Dempster et al.\ in 1977, the \textit{expectation-maximisation} (EM) algorithm is a general iterative method for maximising the likelihood of \textit{latent variables} in statistical models ~\cite{Dempster}. The algorithm contains two stages: the expectation stage, or \textit{E-step}, and the maximisation stage, or \textit{M-step}, which are iterated over until the model converges. The E-step generates a function for the expectation of the likelihood of data points occurring given the current model parameters. The M-step computes new parameters to maximise the function found in the E-step. While this will always increase the \textit{marginal likelihood function}, the EM algorithm does not guarantee convergence on a maximum likelihood estimator, but rather a local maximum. To overcome this, techniques such as \textit{random-restart hill climbing} can be employed ~\cite{HillClimbing}.
\smallskip \\ \indent
Focussing on GMMs, the algorithm can assign observed data points to components of the model such that the likelihood of the components generating the points is maximised. The below process can formalise the E-step. First, the \textit{pseudo-posterior} - the probability that an observation, $X_i$ belongs to a component $Z_k$  - is calculated using Equation \ref{eq:eStep1}.
\begin{align}
    \label{eq:eStep1}
    \gamma_{Z_i = k} = \probP (Z_i = k \; | \; X_i) &= \frac{\probP (X_i \; | \; Z_i = k) \probP(Z_i=k)}{\probP(X_i)} \\
                     &= \frac{\omega_k \mathcal{N}(x_i, \mu_i, \sigma_i)}{\sum_c \omega_c \mathcal{N} (x_c, \mu_c, \sigma_c)} 
\end{align}
where $\omega_k$ is the component weights of component $k$ and $\mathcal{N}(x_i, \mu_i, \sigma_i)$ gives the probability of $x_i$ under component $k$.
\smallskip \\ \indent
The \textit{auxillary function} defined by Equation \ref{eq:eStep2} can then be applied to, $\gamma_{Z_i = k}$, where $\theta_{t-1}$ is the parameter generated in the previous iteration and $\theta_t$ is the new parameter value. Using Jensen's inequality, it can be proven that this auxiliary function is the lower bound of the gain of the likelihood that is obtained by updating the parameter values.
\begin{align}
    \label{eq:eStep2}
    Q(\theta^{(t)}, \theta^{(t-1)}) &= \mathbb{E}\left[ \log \probP(Z \; | \; \theta^{(t)}) \; | \; X, \theta^{(t-1)} \right] \\
    &= \sum^M_{k=1} \log \mathbb{L} (\theta_k \; | \; X, Z) \probP(Z_k | X, \theta^{(t-1)}) \\
    &= \sum^M_{k=1} \log \mathbb{L} (\theta_k \; | \; X, Z) \; \gamma_{Z_i = k}
\end{align}
where $\log \mathbb{L} (\theta_k \; | \; X, Z)$ is the log likelihood of a Gaussian component with updated parameters and $\probP(Z_k | X, \theta^{(t-1)})$ is the distribution of latent variables according to the current parameters.
\smallskip \\ \indent
After the auxiliary function has been generated, the M-step can begin. This means maximising the value of $Q$ to produce the optimal parameter value in Equation \ref{eq:mStep1}.
\begin{equation}
    \label{eq:mStep1}
    \theta^{(t+1)} = argmax_\theta \; Q(\theta^{(t)}, \theta^{(t-1)})
\end{equation}
where
\begin{equation}
    \label{eq:mStep2}
    Q(\theta^{(t)}, \theta^{(t+1)}) = \sum^M_{k=1} \sum^N_{i=1} \log \gamma_k \probP(Z_k \; | \; X_i, \theta^{(t-1)}) + \sum^M_{k=1} \sum^N_{i=1} \log \probP (x_i \; | \; \theta_k) \probP (Z_k \; | \; X_i, \theta^{(t-1)})
\end{equation}
\indent
From this, the optimal parameter values can be derived by differentiating Equation \ref{eq:mStep2} with respect to the means, covariances, and weights, and solving when equal to zero. The results of these calculations are given Equation \ref{eq:mStep3}, Equation \ref{eq:mStep4}, and Equation \ref{eq:mStep5}, respectively, where $N_k = \sum^N_{i=1} \gamma_{Z_i = k}$.
\begin{equation}
    \label{eq:mStep3}
    \hat{\mu}_k = \frac{\sum^N_{i=1} X_i \probP(Z_i = k \; | \; X_i, \theta^{(t-1)})}{\sum^N_{i=1} \probP(Z_i = k \; | \; X_i, \theta^{(t-1)})} = \frac{1}{N_k} \sum^N_{i=1} \gamma_{Z_i = k} \; X_i
\end{equation}
\begin{equation}
    \label{eq:mStep4}
    \hat{\sigma^2}_k = \frac{1}{N_k} \sum^N_{i=1} \gamma_{Z_i=k} (X_i - \mu_k)^2
\end{equation}
\begin{equation}
    \label{eq:mStep5}
    \hat{\omega}_k = \frac{N_k}{N}
\end{equation}






\section{Summary}
\indent \indent
The implementation stage began by implementing a client-server application and then integrating Microsoft's SEAL library to allow HE data transmission, satisfying the first core success criterion. Then, an investigation into optimising the network stack by implementing the \textit{seam carving} algorithm and \textit{graph representations} of images to reduce video size and \textit{parallelisation} to increase the transmission rate.
\smallskip \\ \indent
Afterwards, a novel investigation into adapting moving object detection algorithms for the HE domain was completed. This required modification of algorithms to reduce the number of operations required, development of HE Boolean circuits for more complex operations, and implementation of unsupervised machine learning models.
\smallskip \\ \indent
Finally, a bespoke HE scheme was implemented from first principles following the CKKS scheme initially integrated. This developed understanding and enabled investigation into specialising the implementation as an opportunity for optimisation.


