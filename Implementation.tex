\chapter{Implementation}
\label{chap:implementation}


\section{Architecture}
\setlength{\leftskip}{0.25cm}
\indent \indent
This section is dedicated to detailing the high-level architecture and design of the project. It will discuss the purpose of each component and the software engineering principles applied to make design choices.

\setlength{\leftskip}{0cm}
\subsection{Framework}
\setlength{\leftskip}{0.5cm}
\indent \indent
The project's design began with understanding the MLaaS threat model described in ยง\ref{sec:threatModel}. From this, Figure \ref{fig:abstractNetwork} depicts a high-level layout of the project's core components. The components have been split into two categories: \textit{online} and \textit{offline}. Online describes inference being performed directly in response to a request from the user. Offline describes generating inference results on batches of data, independent of the front-end.
\begin{itemize}
    \item The online components responsible for emulating the MLaaS model are where most of the discussion of this dissertation occurs. The \textit{GUI} allows users to select the encryption scheme and inference method used. The user's video is then passed to the \textit{encryption} component, responsible for encrypting data using the selected scheme - either CKKS or MeKKS. The result is then passed through the \textit{client} to the \textit{server}. In the \textit{inference} component, the data is analysed, and a video containing only the moving objects is returned to the \textit{client} via the \textit{server}. The \textit{decryption} component must then decrypt the inference results and the video played for the user.
    \item The offline components are intended during implementation and evaluation. The \textit{testing} component refers to developing and refining the inference algorithms used to extract moving objects. The \textit{evaluation} component encompasses evaluating the application, including both inference and networking.
\end{itemize}
\indent \indent
Figure \ref{fig:abstractInference} provides insight into the composition of the inference component. The scope of this project only considers the layers above encryption primitives. However, it is important to note that lower layers of abstraction exist. A layer that may be particularly relevant to this investigation is hardware implementation. Hardware modifications could potentially impact the application's performance considerably, both positively and negatively. For example, accelerators, such as GPUs, could be used to perform cryptographic operations ~\cite{Badawi}. Equally, the hardware used in current surveillance implementations may produce weaker results.
\begin{figure}[ht]
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \scalebox{0.5}{\input{figures/abstractNetwork}}
        \captionsetup{justification=centering}
        \caption[Project Components]{The project components.\medskip\\Online:\sethlcolor{ckksRed} \hl{\quad\quad\quad\quad}\smallskip\\Offline:\sethlcolor{mekksBlue} \hl{\quad\quad\quad\quad}}
        \label{fig:abstractNetwork}
    \end{subfigure}%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \scalebox{0.7}{\input{figures/abstractInference}}
        \caption{Abstract layers of the inference stack.}
        \label{fig:abstractInference}
    \end{subfigure}%
    \caption{High-level implementation overview.}
    \label{fig:abstraction}
\end{figure}

\setlength{\leftskip}{0cm}
\subsection{Software Interface}
\setlength{\leftskip}{0.5cm}
\indent \indent
An overview of the project's repository is given in Figure \ref{fig:filetree}. The project was written to clearly distinguish the layers depicted in Figure \ref{fig:abstraction}. The object-oriented approach to design allowed separate components to be implemented independently. As well as aiding comprehension, this architecture was chosen to minimise interaction across abstraction layers and make the project straightforward to expand with more HE schemes or inference methods.
\smallskip \\ \indent
The application can be split into four layers of abstraction, from the high-level interface to the low-level implementation.
\begin{itemize}
    \item The highest level is the graphical user interface that the user interacts with. It allows the encryption scheme and inference method to be configured, and videos to be uploaded.
    \item The next layer contains the networking functionality of the application. Managed by the \texttt{connection} files in both the \texttt{client} and \texttt{server} packages, this layer is responsible for passing any data between the client and the server.
    \item The third layer establishes the API for the cryptographic principles. The HE functionality required by the application is contained within this layer so that the scheme can be easily substituted.
    \item The lowest level contains the cryptographic primitives. Contained within the \texttt{lib} folder, the libraries \texttt{Seal-Python} and \texttt{MeKKS} contain the implementations of these primitives.
\end{itemize}

\begin{figure}[htp]     
    \includegraphics[scale=0.97]{figures/repositoryOverview}
    \caption[Repository Overview]{Repository Overview.}
    \label{fig:filetree}
\end{figure}
\setlength{\leftskip}{0cm}
\subsection{Class Structure}
\setlength{\leftskip}{0.5cm}
\indent \indent
This section provides a more thorough insight into the project's structure. Figure \ref{fig:clientUML} details the arrangement of the client-side, Figure \ref{fig:serverUML} contains the classes composing the server-side, and Figure \ref{fig:mekksUML} depicts the structure of the MeKKS library. While overlaps between diagrams exist, they have been separated into three figures for clarity.

\begin{figure}[htp]
    \centering
    \includegraphics[scale=0.24]{figures/clientClasses}
    \captionsetup{justification=centering}
    % \scalebox{0.6}{\input{figures/clientUML}}
    \caption[Client UML Class Diagram]{UML Class Diagram showing the components of the client side.\medskip\\External CKKS classes:\sethlcolor{ckksRed} \hl{\quad\quad\quad\quad}\smallskip\\External MeKKS classes:\sethlcolor{mekksBlue} \hl{\quad\quad\quad\quad}}
    \label{fig:clientUML}
\end{figure}
        
\begin{figure}[htp]
    \centering
    \includegraphics[scale=0.236]{figures/serverClasses}
    \captionsetup{justification=centering}
    % \scalebox{0.6}{\input{figures/serverClasses}}
    \caption[Client UML Class Diagram]{UML Class Diagram showing the components of the server side.\medskip\\External CKKS classes:\sethlcolor{ckksRed} \hl{\quad\quad\quad\quad}\smallskip\\Abstract HE objects:\sethlcolor{homomorphicGreen} \hl{\quad\quad\quad\quad}}
    \label{fig:serverUML}
\end{figure}
        
\begin{figure}[htp]
    \centering
    \includegraphics[scale=0.19]{figures/mekksClasses}
    \captionsetup{justification=centering}
    % \scalebox{0.6}{\input{figures/mekksUML}}
    \caption[MeKKS UML Class Diagram]{UML Class Diagram showing the components of the MeKKS encryption scheme.}
    \label{fig:mekksUML}
\end{figure}

\setlength{\leftskip}{0cm}





\section{Encryption}
\setlength{\leftskip}{0.25cm}
\indent \indent
This section briefly discusses the fundamentals of the CKKS HE scheme, as described in the original paper by Cheon et al.\ \cite{CKKS}, and how it is implemented in this project.  Also, to provide further insights and an investigation in specialising HE schemes, a bespoke CKKS implementation is detailed, called MeKKS. 

\setlength{\leftskip}{0cm}
\subsection{CKKS Primitives}
\label{sec:CKKS}
\setlength{\leftskip}{0.5cm}
\indent \indent
Fundamentally, the CKKS scheme encrypts plaintext polynomials into ciphertext polynomials. Addition and multiplication operations can then be performed on the data, introducing uncertainty. Ciphertext polynomials can be decrypted to retrieve approximations of plaintext polynomials, with the accuracy depending on the number of operations performed.
\smallskip \\ \indent
To encrypt vectors of real values, they must first be encoded as polynomials in the ring $\mathcal{R} = \mathbb{Z}[X] / (X^N + 1)$, where $N$ is a power of 2. During encoding, the real values must be rounded so precision is preserved by multiplying by a \textit{scaling factor}, $\Delta$. Once the vector has been encoded, it can be encrypted into a \textit{pair} of polynomials: the ciphertext.
\smallskip \\ \indent
Consider two vectors $\vec{v}$ and $\vec{w}$. To encode these vectors, they are scaled to $\Delta \vec{v}$ and $\Delta \vec{w}$.  They can then be encrypted and multiplied to give ciphertext equivalents of $\Delta^2 (\vec{v} \bigodot \vec{w})$. This implies that a sequence of multiplications will increase the scale factor indefinitely. To overcome this, CKKS introduces a \textit{rescaling} procedure, which can be understood as dividing the ciphertext by $\Delta$ to reduce the scale.
\smallskip \\ \indent
However, rescaling is not a free operation, so it cannot be applied to allow unlimited multiplications. CKKS is a levelled HE scheme, so each ciphertext resides at a discrete level. Each level has a coefficient modulus $q$ that dictates a ciphertext's coefficients must be in $\mathbb{Z}_q$. When a polynomial is first encrypted, it exists in the maximum level, $L$, with coefficient modulus $Q = q_0 \dot \Delta^L$, for a \textit{base modulus} $q_0$. When a ciphertext is rescaled, the coefficient modulus is divided by $\Delta$, reducing it to $q_0 \cdot \Delta^{L-1}$. Hence, the ciphertext is lowered to the next level. If a ciphertext reaches level 0, no further multiplications can be applied to preserve the encrypted values. For correct decryption, the coefficients of a polynomial cannot exceed $q_0$. Consequently, in practice, $q_0$ is much larger than $\Delta$.

\setlength{\leftskip}{0cm}
\subsubsection{Encoding and Decoding}
\setlength{\leftskip}{0.5cm}
\indent \indent
The plaintext message space is the \textit{cyclotomic polynomial ring} $\mathcal{R} = \mathbb{Z}[X] / \Phi_M(X)$, where $\Phi_M(X)$ is the $M$-th cyclotomic polynomial and $M$ is a power of two. Encoding is the process of mapping a complex vector to an element in $\mathcal{R}$, and decoding is the process of reversing it.
\smallskip \\ \indent
Another way of defining decoding is as a mapping of an element $r \in \mathcal{R}$ to a vector in $\mathbb{C}^N$ using the embedding $\sigma : \frac{\mathbb{R}[X]}{X^N + 1} \rightarrow \mathbb{C}^N$. This is applied to each root of $\Phi_M(X)$ through the evaluation of $r$ as
\begin{equation}
    \sigma(r) = (r(\xi), r(\xi^3), \ldots, r(\xi^{M-1})) = (r(\xi^k) \: | \: k \in \mathbb{Z}^*_N) \in \mathbb{C}^N
\end{equation}
where $\xi = e^{\frac{2 \pi i}{M}}$ is a primitive $M$-th root of unity.
\smallskip \\ \indent
However, to establish a one-to-one mapping, the input must be \textit{scaled} and \textit{rounded} during encoding, and half of the complex vector must be \textit{discarded} during decoding.
\smallskip \\ \indent
As a result of the above, the CKKS encoding function has the form given in Equation \ref{eq:encode}.
\begin{equation}
    \label{eq:encode}
    \texttt{Encode}(\vec{v}, \Delta) : \mathbb{C}^\frac{N}{2} \times \mathbb{Z}^+ \rightarrow \mathcal{R}
\end{equation}

\setlength{\leftskip}{0cm}
\subsubsection{Operations}
\setlength{\leftskip}{0.5cm}
\indent \indent
A full list of the operations supported by CKKS and their definitions is included in Appendix \ref{app:operations}.


\setlength{\leftskip}{0cm}
\subsubsection{Rescaling}
\setlength{\leftskip}{0.5cm}
\indent \indent
The rescaling function introduced above is defined by equation \ref{eq:rescale}.
\begin{equation}
    \label{eq:rescale}
    \texttt{Rescale}(\vec{c}, \Delta^{l_\text{new}}) : \mathcal{R}^2_{ql} \times \mathbb{Z}^+ \rightarrow \mathcal{R}^2_{ql} = \lfloor \Delta^{l_\text{new} - l} \cdot \vec{c} \rceil \mod q_{l_\text{new}}
\end{equation}
Rescaling truncates some of the least-significant bits of a ciphertext by dividing by a power of the scaling factor. Practically, this is performed after every multiplication to scale $\Delta^2$ down to $\Delta$.

\setlength{\leftskip}{0cm}
\subsubsection{Rotations}
\setlength{\leftskip}{0.5cm}
\indent \indent
An advantage of RLWE based HE schemes over others is the ability to \textit{rotate} ciphertext slots. Given a vector $\vec{V} = [v_0, v_1, v_2, \ldots, v_n]$ and an offset $d$. A rotation would cyclically shift each element by $d$ indices\footnote{For example, given $\vec{v} = [1, 2, 3, 4, 5]$ and $d = 2$ would produce $\vec{v}' = [4, 5, 1, 2, 3]$.}. This is achieved by homomorphically performing a \textit{Galois automorphism} on the ciphertext using standard results from \textit{Galois theory}. However, these operations are significantly the most expensive operations offered by RLWE-based schemes ~\cite{RotationBad}.

\setlength{\leftskip}{0cm}
\subsubsection{RNS Optimisations}
\setlength{\leftskip}{0.5cm}
\indent \indent
SEAL utilises a \textit{residue number system} \cite{RNS} to overcome the slow computations caused by very large polynomial coefficients requiring arbitrary precision arithmetic [RNS]. This exploits the Chinese Remainder Theorem (CRT) to decompose the coefficients in $\mathbb{Z}_q$ into $n$ smaller ones: $\mathbb{Z}_{p_1}, \ldots, \mathbb{Z}_{p_n}$. To do this, the CRT uses a \textit{moduli switching chain} $p_1, \ldots, p_n$ such that $\prod_i p_i = q$ and each $p_i$ is a prime number requiring fewer than 64-bits to store. Thanks to hardware limitations, it is faster to compute the $n$ separate multiplications in $p_1, \ldots, p_n$ than it is to compute a single multiplication in $q$. The results of the separate multiplications can be easily combined thanks to the isomorphism between $\mathbb{Z}_q$ and $\mathbb{Z}_{p_1}, \ldots, \mathbb{Z}_{p_n}$.
\smallskip \\ \indent
Unfortunately, as a result of the RNS optimisation, $q_l = q_0 \Delta^l$ cannot always be maintained due to the difficulty of finding every $q_l$ as the product of $n$ primes. To overcome this, SEAL instead defines $q_l = q_0 \prod_{i=1}^l p_i$ where $q_0$ must be prime. Where the original CKKS scheme divides by $\Delta$ to rescale, SEAL divides by $p_l$. Consequently, the resulting scaling factor is only approximately equal to $\Delta$. Therefore, in practice, the scaling factor must be manually reset to $\Delta$ after each multiplication and rescaling.

\setlength{\leftskip}{0cm}

\subsection{MeKKS}
\label{sec:mekks}
\setlength{\leftskip}{0.5cm}
\indent \indent
Initially, the goal of implementing the CKKS scheme from first principles, named MeKKS, was to understand its fundamental mathematical principles further. There were little to no expectations of providing a performance improvement. However, during implementation, it became apparent that there were advantages to a specialised HE scheme that can overcome weaknesses found in SEAL.

\setlength{\leftskip}{0cm}
\subsubsection{Limitations}
\label{sec:mekksLimitations}
\setlength{\leftskip}{0.5cm}
\indent \indent
The primary limitation of MeKKS was the language used for implementation. Traditionally, encryption schemes are written in C or C++ because they are lower-level than most languages, so they run more efficiently. Consequently, the more computationally expensive operations (such as the multiplications between large prime numbers required by CKKS, detailed above) would run much quicker than in Python. 
\smallskip \\ \indent
Another limitation was the number of optimisations that could be applied. Since the focus of the implementation was on understanding, the implementation began with the foundational CKKS scheme. However, since this scheme was released, many optimisations have been produced. For example, using residue number systems, bootstrapping extensions, or reduced approximation error ~\cite{RNS, BootstrappingHEAAN, RAE}. The opportunities for developing MeKKS are extensive. However, it was essential to define a clear endpoint for the implementation to schedule the project. Therefore, the performance expectations compared to SEAL were significantly bounded.x

\setlength{\leftskip}{0cm}
\subsubsection{Implementation}
\setlength{\leftskip}{0.5cm}
\indent \indent
The first iteration of MeKKS implementation began using the scheme outlined by Cheon et al.\ in 2016 ~\cite{CKKS}. This allowed all of the functionality described in ยง\ref{sec:CKKS} to be implemented following the SEAL API, allowing for easy integration with the existing application.
\smallskip \\ \indent
However, the performance of this implementation meant it was infeasible to integrate into the rest of the application. Therefore, the bootstrapping procedure from the next HEAAN iteration was used to speed up computation ~\cite{BootstrappingHEAAN}. Bootstrapping takes advantage of the \textit{approximate computation} characteristic of HEAAN to evaluate the decryption formula approximately so an encryption of the original message can be obtained in a large ciphertext modulus. Hence, an approximation of the \textit{modular reduction} formula is implemented such that it can be efficiently evaluated using standard operations - where the error induced by the approximation is small enough to maintain precision.
\smallskip \\ \indent
Using the observation that the modular reduction function, $F(t) = [t]_q$, is the identity near zero and periodic in $q$, bootstrapping uses a trigonometric function to approximate the function when $t = \langle \texttt{Ciphertext}, \texttt{SecretKey} \rangle$ is close to a multiple of $q$ (the ciphertext modulus). Specifically, using the \textit{sine} function given by Equation \ref{eq:bootstrapping}.
\begin{equation}
    \label{eq:bootstrapping}
    [\langle \texttt{Ciphertext}, \texttt{SecretKey} \rangle]_q = \frac{q}{2 \pi} \cdot \sin \left( \frac{2 \pi}{q} \cdot \langle \texttt{Ciphertext}, \texttt{SecretKey} \rangle \right) + O(\epsilon^3 \cdot q)
\end{equation}
when $F(\langle \texttt{Ciphertext}, \texttt{SecretKey} \rangle) < \epsilon \cdot q$.
\smallskip \\ \indent
The Taylor polynomial can be used to approximate the trigonometric function so that it can be evaluated in the HE domain. The input, $t$, is bounded by $K \cdot q$ for some constant $K = O(\lambda)$, where $\lambda$ is the security parameter. The degree of the Taylor polynomial should be at least $O(K \cdot q)$ in order to make the error term small enough on the interval $(-K \cdot q, K \cdot q)$. Cheon et al.\ proposed using the \textit{Paterson-Stockmeyer} method to reduce the complexity of these calculations ~\cite{BootstrappingHEAAN, Paterson}. However, the complexity of recryption grows exponentially with the depth of the decryption circuit, which still has a substantial impact.
\smallskip \\ \indent
Figure \ref{fig:bootstrapping} provides a graphical representation of the bootstrapping approximation.
\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.3]{figures/bootstrapping.png}
    \caption[Bootstrapping Procedure]{Modular Reduction and Scaled Sine Functions. Reproduced from \cite{BootstrappingHEAAN}.}
    \label{fig:bootstrapping}
\end{figure}

\setlength{\leftskip}{0cm}
\subsubsection{Optimisations}
\setlength{\leftskip}{0.5cm}
\indent \indent
The principal optimisations of MeKKS came through data representation. When integrating SEAL, a Python wrapper for a C++ implementation was used. Consequently, the application was forced to handle large C++ objects, abstracted through the wrapper. Therefore, Python struggled to manipulate these objects efficiently, specifically when serialising data for transmission. However, since MeKKS was written in Python, the underlying functionality provided much more efficient manipulation as it could directly interact with the objects and their attributes.
\smallskip \\ \indent
Another optimisation came through the specialisation of the library. The project schedule ensured MeKKS was only implemented once the application's core had been complete. As a result, the functionality required had been finalised and only needed operations were implemented for MeKKS. This further reduced the size of objects and removed any unnecessary computations, making execution more efficient. For example, the most expensive operations offered by SEAL are rotations. However, since the application did not use them, they were ignored during the implementation of MeKKS.

\setlength{\leftskip}{0cm}



\section{Networking}
\label{sec:networking}
\setlength{\leftskip}{0.25cm}
\indent \indent
This section details the techniques investigated for optimising communication between the client and server. An established limitation of HE is the size of ciphertexts ~\cite{Makkaoui}. Consequently, the \textit{transmission time} of data is significantly impaired. The \textit{transmission time} can be defined by Equation \ref{eq:transmission}.
\begin{equation}
    \text{transmission time} = \frac{\text{video size }\textit{(bytes)}}{\text{transmission rate }\textit{(bytes/second)}}
    \label{eq:transmission}
\end{equation}
\indent \indent
Transferring large volumes of data to the cloud is a critical component of the MLaaS model, so a substantial portion of the investigation was dedicated to reducing transmission time. The problem was considered from two angles: attempting to reduce the \textit{video size} and attempting to increase the \textit{transmission rate}.

\setlength{\leftskip}{0cm}
\subsection{Video Size}
\subsubsection{Seam Carving}
\label{sec:seamCarving}
\setlength{\leftskip}{0.5cm}
\indent \indent
Developed by Avidan and Shamir in 2007, \textit{seam carving} describes a method of resizing images using \textit{geometric constraints} while also considering \textit{image content} ~\cite{SeamCarving}. Consequently, an image can be resized while preserving important features, such as people or buildings. There are two categories for distinguishing these features. Firstly, \textit{top-down} methods use tools such as \textit{face detectors} to highlight were the features appear in the image ~\cite{Viola}. Whereas a \textit{bottom-up} approach uses saliency maps\footnote{a representation highlighting the regions of an image where a person's eyes are first drawn, see ~\cite{Saliency}.} to locate the most important ~\cite{Itti}.
\smallskip \\ \indent
The details of the original seam carving paper, as well as a depiction of the algorithm, are included in Appendix \ref{app:seamCarving}. However, a more advanced algorithm was implemented for this investigation to provide more optimal results.
\smallskip \\ \indent
To quantify the importance of a pixel, seam carving defines an \textit{energy function}. Rubinstein et al.\ \cite{Rubinstein} proposed the \textit{forward energy} function using dynamic programming. This method calculates the energy of a pixel by accounting for the impact on future energies if it is removed. To achieve this, the \textit{energy difference} function is defined by Equation \ref{eq:energyDiff}. The cost of removing pixels, $C$, is measured as the forward differences between the pixels that would become neighbours after deletion. There are three cases for this: diagonally adjacent in each direction and orthogonally adjacent, depicted by Figure \ref{fig:adjacency} and defined by Equation \ref{eq:adjacency}.
\begin{equation}
    \label{eq:energyDiff}
    \Delta E_{i+1} = E(\vec{I}_{i+1}) - (E(\vec{I}_i) - E(C_i))
\end{equation}
\begin{figure}
    \scalebox{0.78}{\input{figures/forwardEnergy}}
    \caption[Potential Seam Costs]{There are three possible vertical seam costs for pixel $(i, j)$. After removing the seam, new neighbours (blue) and new edges (red) are created.}
    \label{fig:adjacency}
\end{figure}
\begin{align}
    \label{eq:adjacency}
    \begin{split}
        C_L(i, j) &= |\vec{I}(i, j+1) - \vec{I}(i, j-1)| + |\vec{I}(i-1, j) - \vec{I}(i, j-1)|
    \end{split} \\
    \begin{split}
        C_U(i, j) &= |\vec{I}(i, j+1) - \vec{I}(i, j-1)|
    \end{split} \\
    \begin{split}
        C_R(i, j) &= |\vec{I}(i, j+1) - \vec{I}(i, j-1)| + |\vec{I}(i-1, j) - \vec{I}(i, j+1)|
    \end{split}
\end{align}
\indent
Once the energy has been calculated, the image can be split into \textit{seams}. A \textit{vertical seam} is a path of pixels connecting the top of an image to the bottom, such that there is only a single pixel from each row in the path. Likewise, a \textit{horizontal seam} connects the left of an image to the right, such that only a single pixel from each column is included. Formally, this is defined by Equation \ref{eq:vSeam} and Equation \ref{eq:hSeam} respectively.
\begin{subequations}
    \begin{equation}
        \label{eq:vSeam}
        \vec{s}^\vec{x} = \{s^x_i\}^n_{i=1} = \{(x(i), i) \: s.t. \: \forall i \: |x(i) - x(i-1)| \leq 1\}^n_{i=1}
    \end{equation}
    \begin{equation}
        \label{eq:hSeam}
        \vec{s}^\vec{y} = \{s^y_j\}^m_{j=1} = \{(j, y(j)) \: s.t. \: \forall j \: |y(j) - y(j-1)| \leq 1\}^m_{j=1}
    \end{equation}
\end{subequations}
for an $n \times m$ image, \vec{I}, where $x : [1, \ldots, n] \rightarrow [1, \ldots, m]$ and $y : [1, \ldots, m] \rightarrow [1, \ldots, n]$.
\smallskip \\ \indent
Using these definitions and the energy function, the \textit{optimal seam} can be found. That is, the seam that minimises the \textit{seam cost} - the sum of all pixel costs in the path. Some implementations will use variants of Dijkstra's algorithm for this. Alternatively, Equation \ref{eq:forwardEnergy} defines a dynamic programming approach.
\begin{equation}
    \label{eq:forwardEnergy}
    M(i, j) = e(i, j) + \min 
    \begin{cases}
        M(i-1, j-1) + C_L(i,j) \\
        M(i-1, j) + C_U(i,j) \\
        M(i-1, j+1) + C_R(i,j)
    \end{cases}
\end{equation}
\indent
It is important to note that there have been several extensions to seam carving that may apply to this project. Particularly, optimisations for videos by introducing two-dimensional seams to allow time to be accounted for, and implementations using GPUs to reduce execution time ~\cite{Rubinstein, Duarte}.

\setlength{\leftskip}{0cm}
\subsubsection{Graph Representation}
\label{sec:graphReps}
\setlength{\leftskip}{0.5cm}
\indent \indent
Representing images using graphs has several advantages. Firstly, graphs are discrete, mathematically simple objects with an established set of provably correct algorithms. More pertinent to this investigation, graphs provide flexible representations that can be used to tune image size.
\smallskip \\ \indent
Graph-based image processing methods operate on \textit{pixel adjacency graphs} - graphs whose vertex set is the set of image pixels and edge set defines adjacency of pixels. An example of some pixel adjacency graphs is given by Figure \ref{fig:pixelAdjacency}. Three-dimensional pixel adjacency graphs account for relationships between video frames when handling video files. An example of these is depicted by Figure \ref{fig:3dAdjacency}.
\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.29\textwidth}
        \centering
        \captionsetup{justification=centering}
        \scalebox{1}{\input{figures/adjacency1}}
        \caption{A 2D image with $4 \times 4$ pixels.}
    \end{subfigure} \hfill%
    \begin{subfigure}[b]{0.29\textwidth}
        \centering
        \captionsetup{justification=centering}
        \scalebox{1}{\input{figures/adjacency2}}
        \caption{A 4-connected pixel adjacency graph.}
    \end{subfigure} \hfill%
    \begin{subfigure}[b]{0.29\textwidth}
        \centering
        \captionsetup{justification=centering}
        \scalebox{1}{\input{figures/adjacency3}}
        \caption{An 8-connected pixel adjacency graph.}
    \end{subfigure}%
    \caption[Pixel Adjacency Graphs]{Pixel adjacency graphs.}
    \label{fig:pixelAdjacency}
\end{figure}
\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.29\textwidth}
        \centering
        \captionsetup{justification=centering}
        \scalebox{1}{\input{figures/3dAdjacency1}}
        \caption{A video with three frames of $3 \times 3$ pixels.}
    \end{subfigure} \hfill%
    \begin{subfigure}[b]{0.29\textwidth}
        \centering
        \captionsetup{justification=centering}
        \scalebox{0.5}{\input{figures/3dAdjacency2}}
        \caption{A 6-connected 3D pixel adjacency graph.}
    \end{subfigure} \hfill%
    \begin{subfigure}[b]{0.29\textwidth}
        \centering
        \captionsetup{justification=centering}
        \scalebox{0.5}{\input{figures/3dAdjacency3}}
        \caption{An 18-connected 3D pixel adjacency graph.}
    \end{subfigure}%
    \caption[3D Pixel Adjacency Graphs]{3D pixel adjacency graphs.}
    \label{fig:3dAdjacency}
\end{figure}
\smallskip \\ \indent
However, to improve the video size, pixel adjacency graphs must be extended to \textit{region adjacency graphs}. In this case, rather than representing each pixel with a node, pixels are amalgamated into regions represented by a single node. Figure \ref{fig:pixelToRegion} provides a pictorial example of this.
\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \captionsetup{justification=centering}
        \scalebox{0.5}{\input{figures/pixelToRegion1}}
        \caption{A pixel adjacency graph.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \captionsetup{justification=centering}
        \scalebox{0.5}{\input{figures/pixelToRegion2}}
        \caption{The corresponding region adjacency graph.}
    \end{subfigure}
    \caption[Pixel to Region Conversion]{Converting a pixel adjacency graph to a region adjacency graph.}
    \label{fig:pixelToRegion}
\end{figure}
\smallskip \\ \indent
To achieve this, the notion of similarity between pixels must be quantified. Perhaps surprisingly, this is another image segmentation problem. Consequently, established algorithms producing valid solutions exist. Unsupervised clustering algorithms such as \textit{the watershed transform} \cite{Watershed} or \textit{k-means clustering} \cite{KMeans} are two such methods that have proved useful in existing works.
\smallskip \\ \indent
Using similar techniques to seam carving, it is possible to make this trade-off less severe. For example, \textit{Foveal sampling} is a method of recreating the visual activity of the eye when determining regions ~\cite{Foveal}. The \textit{Fovea centralis} is a region of the retina responsible for the sharp central vision used by mammals to focus on particular objects. Consequently, its shape can be used to bias the placement of nodes of a graph to prioritise more critical areas. This allows the region budget to be used more efficiently to reduce noticeable quality reduction. Other techniques have been developed using saliency maps or similar.
\begin{figure}[h!]
    \centering
    \begin{subfigure}[t]{0.29\textwidth}
        \centering
        \captionsetup{justification=centering}
        \includegraphics[scale=0.34]{figures/adjacencyBefore}
        \caption{A photograph of a cup of coffee.}
    \end{subfigure} \hfill%
    \begin{subfigure}[t]{0.29\textwidth}
        \centering
        \captionsetup{justification=centering}
        \includegraphics[scale=0.34]{figures/adjacencyMiddle}
        \caption{The picture of coffee split into approximately 400 regions.}
    \end{subfigure} \hfill%
    \begin{subfigure}[t]{0.29\textwidth}
        \centering
        \captionsetup{justification=centering}
        \includegraphics[scale=0.34]{figures/adjacencyAfter}
        \caption{The picture of coffee split into approximately 10 regions.}
    \end{subfigure}%
    \caption[Tuning Region Adjacency Graphs]{A demonstration of the impact of reducing the number of regions on image quality. Images taken from \cite{Foveal}.}
    \label{fig:regions}
\end{figure}
\smallskip \\ \indent
Using similar techniques to seam carving, it is possible to make this trade-off less severe. For example, \textit{Foveal sampling} is a method of recreating the visual activity of the eye in the mapping of an image ~\cite{Foveal}. The \textit{Fovea centralis} is a region of the retina responsible for the sharp central vision used by mammals to focus on particular objects. Foveal sampling uses the shape of the Fovea centralis to produce a graph that can be overlayed onto an image, extracting the areas that an observer will focus on. Consequently, more regions are created in areas critical to perception, and fewer in areas out of focus. This allows the region budget to be more efficiently used, so the overall number required can be smaller without impacting image quality as significantly. Similar techniques have been applied using saliency maps or other methods for determining the importance of regions in an image.
\begin{figure}[htp]
    \centering
    \begin{subfigure}[t]{0.29\textwidth}
        \centering
        \captionsetup{justification=centering}
        \includegraphics[scale=0.34]{figures/fovealBefore}
        \caption{A photograph of some ducks.}
    \end{subfigure} \hfill%
    \begin{subfigure}[t]{0.29\textwidth}
        \centering
        \captionsetup{justification=centering}
        \includegraphics[scale=0.34]{figures/fovealMap}
        \caption{The retinal topography of a kangaroo.}
    \end{subfigure} \hfill%
    \begin{subfigure}[t]{0.29\textwidth}
        \centering
        \captionsetup{justification=centering}
        \includegraphics[scale=0.34]{figures/fovealResult}
        \caption{The photograph of ducks sampled using the retinal topography of a kangaroo.}
    \end{subfigure}%
    \caption[Foveal Sampling]{An example of foveal sampling. Images taken from \cite{Foveal}.}
    \label{fig:foveal}
\end{figure}

\setlength{\leftskip}{0cm}
\subsection{Transmission Rate}
\label{sec:parallelisation}
\setlength{\leftskip}{0.5cm}
\indent \indent
Where the previous sections aimed to improve video transmission time by reducing the size of video files, this section targets the bottlenecks limiting the transmission rate of the system. To do this, the project investigates the application of \textit{parallel} and \textit{concurrent} computing.
\smallskip \\ \indent
Parallel computing is often conflated with \textit{concurrent computing}. However, the terms are distinct. Parallel computing means a task is broken down into numerous similar sub-tasks that can be completed independently ~\cite{ParallelismVsConcurrency}. Concurrent computing means each sub-task will address unrelated processes, often requiring inter-task communication ~\cite{ParallelismVsConcurrency}. In Figure \ref{fig:parallelStack}, the abstract layers of the networking processes have been coloured to indicate whether concurrent or parallel computing is used.
\begin{figure}[htp]
    \centering
    \scalebox{0.6}{\input{figures/parallelStack}}
    \captionsetup{justification=centering}
    \caption[Abstract view of parallel processes]{The stages required for a video to be sent across the network.\medskip\\Parallel processes:\sethlcolor{ckksRed} \hl{\quad\quad\quad\quad}\smallskip\\Concurrent processes:\sethlcolor{mekksBlue} \hl{\quad\quad\quad\quad}}
    \label{fig:parallelStack}
\end{figure}
\smallskip \\ \indent
These techniques were selected for investigation because of the growing trend of support in computer architecture. Traditionally, computer design has focussed on \textit{sequential computation} to improve performance. However, factors such as Dennard scaling \cite{Dennard} mean the improvements predicted by Moore's law \cite{Moore} may not continue indefinitely. Therefore, architects are utilising multiprocessing to gain similar gains. Consequently, this seemed like a viable opportunity for the investigation to consider future iterations of surveillance technology.

\setlength{\leftskip}{0cm}
\subsubsection{Communication}
\label{sec:communication}
\setlength{\leftskip}{0.5cm}
\indent \indent
Parallelisation already exists in some communication protocols. The \textit{transmission control protocol} (TCP) uses a \textit{sliding window protocol} to send a group of data packets concurrently, ensuring they are ordered correctly at the receiving end. Figure \ref{fig:slidingWindow} depicts this. This and similar protocols exist in the \textit{data-link} layer of the \textit{OSI network model}. The goal of this section of the investigation was to attempt to move the parallelisation higher up the abstract stack.
\begin{figure}[htp]
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[scale=0.3]{figures/slidingWindow1}
        \caption{The sliding window (red) moves across the packets as each one is sent. Initially the first four packets are sent (top). When packet A is acknowledged, the window slides along one, and E is sent (middle). After the acknowledgement for B is received the window will slide along and F will eventually be sent (bottom).}
    \end{subfigure}\hfill%
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[scale=0.35]{figures/slidingWindow2}
        \caption{The packets currently in the window are sent at the same time, without waiting for any acknowledges. Once an acknowledgment has been received, the next packet in the queue can be sent.}
    \end{subfigure}%
    \caption[The Sliding Window Protocol]{A high-level view of TCP's sliding window protocol.}
    \label{fig:slidingWindow}
\end{figure}
\smallskip \\ \indent
Taking inspiration from sliding windows, instead of sending all video data in a single stream, videos are split into frames, and each frame is divided into packets. Meanwhile, a pool of threads can be created to represent the size of the window. When a packet is ready to be sent, a thread is assigned and establishes a connection with the server. Consequently, multiple connections will be open in parallel, allowing more data to be sent. 
\smallskip \\ \indent
However, there are limitations to this technique. Firstly, more data will have to be transmitted than in sequential communication. The algorithm is non-deterministic, so there can be no guarantees about the order in which the packets will arrive after transmission. Consequently, further information must be provided to ensure videos are reassembled correctly. While this is worth noting, the size of this additional data is negligible compared to HE data, so it is not a critical issue. 
\smallskip \\ \indent
A more pressing concern is the overhead of creating threads and establishing connections. The cost is such that creating too many threads will remove parallelisation benefits or make transmission slower. Consequently, an optimal balance between the cost of parallelisation and the amount of data to send must be found to maximise gains from this approach.

\setlength{\leftskip}{0cm}
\subsubsection{Data Manipulation}
\setlength{\leftskip}{0.5cm}
\indent \indent
Splitting videos into small packets has further advantages. Before data can be transmitted, it must be prepared, or \textit{packed}. Equivalently, when data arrives at its destination, data must be \textit{unpacked}.
\smallskip \\ \indent
Depicted by Figure \ref{fig:packingAndUnpacking}, there are three distinct stages of packing in the client \textit{encryption}, \textit{compression}, and \textit{serialisation}. The unpacking process will reverse these stages in order. In the server-side of the project, the encryption and decryption operations are unsurprisingly missing from these pipelines.
\begin{figure}[htp]
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \scalebox{0.8}{\input{figures/packing}}
        \caption{The packing stack.}
        \label{fig:packing}
    \end{subfigure}%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \scalebox{0.8}{\input{figures/unpacking}}
        \caption{The unpacking stack.}
        \label{fig:unpacking}
    \end{subfigure}%
    \caption[Packing and Unpacking]{The packing and unpacking stacks. Block colours indicate processes occur in both client and server, patterned colours indicate the process is only occurs in the client.}
    \label{fig:packingAndUnpacking}
\end{figure}
\smallskip \\ \indent
In a na\"ive implementation, each video pixel might be packed individually. However, this can be improved. The CKKS scheme operates on vectors of real values. Therefore, decomposing a frame into rows provides the opportunity for \textit{vectorising} the application by encrypting each row as a single ciphertext object. Consequently, the number of ciphertexts needed is reduced - for an $n \times m$ pixel frame, the number of objects is reduced from $nm$ to $n$ - so the time and space complexity of encryption is reduced from quadratic to linear complexity\footnote{This would fall under concurrency rather than parallelism because objects such as encoders, encryptors, and keys must be shared between processes.}.
\smallskip \\ \indent
Similarly, compressing and serialising subsections of video can be parallelised to improve time and space complexity. Therefore, once encryption has completed, these operations can be performed by the same thread. No further overhead of thread creation is required, so this improvement is only limited by the need for each stage to terminate entirely before the next begins. Consequently, the smaller the quantum of data they operate on, the quicker they will complete. However, smaller quanta requires more threads, so a similar balance to that described in ยง\ref{sec:communication} must be achieved.

\setlength{\leftskip}{0cm}




\section{Inference}
\label{sec:inference}
\setlength{\leftskip}{0.25cm}
\indent \indent
This section discusses the implementation of the moving object detection algorithms. It will examine the necessary modifications to support HE video data in ยง\ref{sec:adaptations}, and detail the more complex algorithms needed for unsupervised learning in ยง\ref{sec:OMM} and ยง\ref{sec:EMAlg}. Adaptations were required to incorporate the HE Boolean circuits and overcome operation depth limitations. 

\setlength{\leftskip}{0cm}
\subsection{Homomorphic Encryption Adaptations}
\label{sec:adaptations}
\setlength{\leftskip}{0.5cm}
\indent \indent
There were two main challenges to overcome when implementing inference algorithms in the HE domain. Firstly, the number of operations that can be applied is limited by the depth of the ciphertext. Secondly, the set of operations supported by HE is more limited than is available when working with plain data. Consequently, compromises were made to produce accurate results without introducing detrimental side effects - for example, to accommodate more operations, the depth of a ciphertext could be increased, but this significantly increases the memory usage of ciphertexts, so data transmission quickly becomes infeasible.
\smallskip \\ \indent
The adaptations implemented for the less complex algorithms are detailed below. A discussion of the techniques investigated for implementing GMMs is evaluated in ยง\ref{sec:integration}.

\setlength{\leftskip}{0cm}
\subsubsection{Frame Differencing}
\setlength{\leftskip}{0.5cm}
\indent \indent
Frame differencing is a relatively simple algorithm to adapt for the HE domain. It only requires a single operation, \textit{subtraction}, that is provided by the standard CKKS implementation. Moreover, subtraction does not impact the level of a ciphertext, so has no impact on ciphertext size. As well as making networking more efficient, this also makes the inference algorithm faster because the data being operated over is smaller.
\smallskip \\ \indent
Therefore, the only modification required to operate over HE data is to replace the subtraction function in the traditional algorithm with a call to the subtraction circuit provided by the HE library.


\setlength{\leftskip}{0cm}
\subsubsection{Mean Filter}
\setlength{\leftskip}{0.5cm}
\indent \indent
The most straightforward implementation recalculates the reference frame repeatedly by storing a list of all frames that have been observed and finding their mean each time a new frame is received. In contrast, a more advanced implementation will use an iterative function to update the mean, so does not require storing all frames.
\smallskip \\ \indent
Evidently, the second method will perform better in both time and space complexity when considering plain video data. However, when investigating HE data, the distinction is not as clear. The second method becomes problematic in that the mean must have both \textit{multiplication} and \textit{addition} operations applied to it during the updating phase. Each time a multiplication circuit is applied, the ciphertext will be reduced. Therefore, the ciphertext must have the same number of levels as frames in the video. This quickly becomes infeasible because increasing ciphertext size so far would severely increase the time complexity of operations and make transmission times impracticably slow. Consequently, this method can be immediately ruled out
\smallskip \\ \indent
The first method encounters different difficulties. One particular issue is the space complexity of storing all frames in the video. Since HE data can get very large, storing multiple copies of each frame significantly impacts the application's memory usage. Similarly, HE operations are noticeably slower than plaintext operations. Therefore, while the cost of recalculating the mean of plaintext data may be relatively negligible, as more frames are observed, HE implementations will become considerably slower. However, this method can be used to derive a solution. A compromise can be achieved by limiting the number of frames stored, forgetting the oldest frame whenever a new one arrives. Importantly, this may reduce inference accuracy, so a balance must be struck between running time and inference quality.

\setlength{\leftskip}{0cm}
\subsubsection{Gaussian Average}
\setlength{\leftskip}{0.5cm}
\indent \indent
Similarly to a mean filter, a plaintext implementation of Gaussian average inference would fit the Gaussian distributions using iterative formulae for the mean and variance. Copied from ยง\ref{sec:gaussianAverage} for convenience, Equation \ref{eq:mean2} and Equation \ref{eq:variance2} provide these formulae. Consequently, the plaintext implementation will be infeasible in the HE domain. Fortunately, it can be adapted.
\begin{equation}
    \label{eq:mean2}
    \mu_t =
    \begin{cases}
        f_0 & \text{if $t = 0$} \\
        \alpha f_t + (1 - \alpha) \mu_{t-1} & \text{otherwise}
    \end{cases}
\end{equation}
\begin{equation}
    \label{eq:variance2}
    \sigma^2_t =
    \begin{cases}
        c & \text{if $t = 0$} \\
        d^2 \alpha + (1 - \alpha) \sigma^2_{t-1} & \text{otherwise}
    \end{cases}
\end{equation}
where $\alpha$ determines the size of the temporal window, $d = |f_t - \mu_t|$ represents the Euclidean distance between a pixel and the mean, and $c$ is some constant.
\smallskip \\ \indent
Firstly, it can be observed that expanding the iterative formulae highlights that the frames of a video are \textit{multiplicatively independent}, as demonstrated by Equation \ref{eq:expansion} for the fifth frame of the video.
\begin{equation}
    \label{eq:expansion}
    \begin{split}
        \mu_4 &= \alpha f_4 + \alpha (1-\alpha) f_3 + \alpha (1-\alpha)^2 f_2 + \alpha (1-\alpha)^3 f_1 + (1-\alpha)^4 f_0 \\ 
        \sigma^2_4 &= \alpha d^2_4 + \alpha (1-\alpha) d^2_3  + \alpha (1-\alpha)^2 d^2_2 + \alpha (1-\alpha)^3 d^2_1 + (1-\alpha)^4 c
    \end{split}
\end{equation}
\indent
Helpfully, the coefficient terms across calculations are identical, so computation can be shared. More importantly, the server decides the value of $\alpha$ before runtime. Therefore, the coefficients can be pre-calculated in the clear before being encoded for HE multiplication. This means that only a single multiplication needs to be applied to each frame. Then, results can be summed to give the mean and variance. Consequently, the number of levels required for a ciphertext is significantly reduced, so time and space complexity are also reduced.
\smallskip \\ \indent
However, this method still requires storing a list of preceding frames to recalculate terms repeatedly. Like with mean filter, this introduces a trade-off between running time and memory usage against inference accuracy.

\setlength{\leftskip}{0cm}
\subsection{Online Mixture Model}
\label{sec:OMM}
\setlength{\leftskip}{0.5cm}
\indent \indent
In 1999 Stauffer and Grimson proposed \textit{adaptive background mixture models} for real-time moving object detection ~\cite{Stauffer}. To overcome a single Gaussian distribution's inability to cope with the changing lighting conditions in practice, they proposed a mixture of adaptive Gaussians. The advantage of this technique over other GMMs is that the model runs \textit{online} because no training phase is required. Instead, the model can be fitted, and results returned in a single phase. While this is useful for real-time inference acting on a constant stream of data, it has the disadvantage of producing less accurate results earlier in the execution sequence.

\setlength{\leftskip}{0cm}
\subsubsection{Fitting}
\setlength{\leftskip}{0.5cm}
\indent \indent
For a particular pixel, the values that occur over time are known as the \textit{pixel process}. This is a time series of pixel values such that, at any time $t$, the process of pixel $(x,y)$ is defined by Equation \ref{eq:pixelProcess}.
\begin{equation}
    \label{eq:pixelProcess}
    \{X_0, \ldots, X_t\} = \{I(x,y,i) \; | \; 0 \leq i \leq t\}
\end{equation}
where $I$ is the image sequence.
\smallskip \\ \indent
The history of a pixel can be modelled as a mixture of $K$ Gaussian distributions. $K$ is usually a value between 3 and 5, depending on available memory and computational power availability. Given a pixel process, the probability of observing the pixel value at time $t$ is given by
\begin{equation}
    \probP (X_t) = \sum^K_{i=1} \omega_{i,t} \times \eta(X_t, \mu_{i,t}, \Sigma_{i,t})
\end{equation}
where $\omega_{i,t}$ represents an estimate of the proportion of the data accounted for by the $i^{\text{th}}$ Gaussian at time $t$, $\mu_{i,t}$ and $\Sigma_{i,t}$ are the mean and covariance matrix of the $i^{\text{th}}$ Gaussian at time $t$ respectively. $\eta$ is the Gaussian probability density function given by Equation \ref{eq:pdf}.
\begin{equation}
    \label{eq:pdf}
    \eta (X, \mu, \Sigma) = \frac{1}{(2\pi)^\frac{n}{2} |\Sigma|^\frac{1}{2}} e^{-\frac{1}{2} (X - \mu)^T \Sigma^{-1} (X - \mu)}
\end{equation}
\indent
To maximise the likelihood of the observed data, a \textit{k-means} approximation was selected to engender the online aspect of the system. Each pixel in a new frame is compared against the existing $K$ Gaussian distributions until a \textit{match} is found. A match occurs when a pixel value is within a predefined number of standard deviations of a distribution. The number of standard deviations will vary across distributions as each distribution will account for different factors such as lighter or shadier regions. 
\smallskip \\ \indent
If none of the distributions match a pixel's value, the least likely Gaussian is replaced by a new distribution defined with the current value as its mean, an initially high variance, and low prior weight. Then, the prior weights are adjusted at time $t$ using Equation \ref{eq:priors}.
\begin{equation}
    \label{eq:priors}
    \omega_{k,t} = (1-\alpha)\omega_{k,t-1} + \alpha M_{k,t}
\end{equation}
where $\alpha$ is a learning rate, and $M$ is an indicator function of $1$ if Gaussian $k$ at time $t$ matched, and $0$ otherwise. After this approximation is complete, the weights are normalised.
\smallskip \\ \indent
For unmatched distributions, the $\mu$ and $\sigma$ parameters are unchanged. However, the parameters of the matching distributions are updated according to Equation \ref{eq:muAndSigma}, where $rho$ is defined by Equation \ref{eq:rho}.
\begin{subequations}
    \label{eq:muAndSigma}
    \begin{equation}
        \mu_t = (1 - \rho) \mu_{t-1} + \rho X_t
    \end{equation}
    \begin{equation}
        \Sigma^2_t = (1 - \rho) \Sigma^2_{t-1} + \rho(X_t - \mu_t)^T(X_t - \mu_t)
    \end{equation}
\end{subequations}
\begin{equation}
    \label{eq:rho}
    \rho = \alpha \eta(X_t, \mu_k, \Sigma_k)
\end{equation}

\setlength{\leftskip}{0cm}
\subsubsection{Predicting}
\setlength{\leftskip}{0.5cm}
\indent \indent
Once the parameters have been updated, each pixel must be labelled by determining the Gaussian component most likely produced by the background process. This decision assumes that there will be relatively little variance in the Gaussians when a static object is in the frame and a large variance not matching existing distributions when a new object occludes the background. Consequently, a method defining the proportion of the GMM representing the background process is required.
\smallskip \\ \indent
To achieve this, first, the Gaussians are ordered based on the value of $\frac{\omega}{\Sigma}$. The definitions of $\omega$ and $\Sigma$ mean that this value will increase as the distribution gains more evidence and the variance decreases. This value will only differ from the last iteration for matching distributions, so the sorting process can be made more efficient. The ordered list can then be iterated over, and the first $B$ distributions are taken as the \textit{background model}, where 
\begin{equation}
    \label{eq:gmmInequality}
    B = argmin_b \left( \sum^b_{k=1} \omega_k > T \right)
\end{equation}
The threshold, $T$, is a measure of how much data should be accounted for. In other words, the best-fitted distributions are taken until a certain portion of recent data has been considered. 
\smallskip \\ \indent
Once the background model has been decided, it can be used to label the pixel as either \textit{foreground} or \textit{background}, allowing the moving objects to be extracted as the foreground.

\setlength{\leftskip}{0cm}
\subsection{Expectation-Maximisation Algorithm}
\label{sec:EMAlg}
\setlength{\leftskip}{0.5cm}
\indent \indent
Proposed by Dempster et al.\ in 1977, the \textit{expectation-maximisation} (EM) algorithm is a general iterative method for maximising the likelihood of \textit{latent variables} of a statistical model ~\cite{Dempster}. There are two stages in the algorithm: the expectation stage, or \textit{E-step}, and the maximisation stage, or \textit{M-step}, which are iterated over until the model converges. The E-step generates a function for the expectation of the likelihood of data points occurring given the current model parameters. The M-step computes new parameters to maximise the function found in the E-step. While this will always increase the \textit{marginal likelihood function}, there is no guarantee that the EM algorithm will converge to a maximum likelihood estimator: the algorithm may converge on a local maximum. To overcome this, techniques such as \textit{random-restart hill climbing} can be employed ~\cite{HillClimbing}.
\smallskip \\ \indent
Although the EM algorithm can be applied to any statistical model, this dissertation will discuss its application to GMMs. The algorithm can be used to assign observed data points to components of the model such that the likelihood of the components generating the points is maximised. When applied to a GMM, the E-step can be formalised by the below process. To begin with, the \textit{pseudo-posterior} - the probability that an observation, $X_i$ belongs to a component $Z_k$  - is calculated using Equation \ref{eq:eStep1}.
\begin{align}
    \label{eq:eStep1}
    \gamma_{Z_i = k} = \probP (Z_i = k \; | \; X_i) &= \frac{\probP (X_i \; | \; Z_i = k) \probP(Z_i=k)}{\probP(X_i)} \\
                     &= \frac{\omega_k \mathcal{N}(x_i, \mu_i, \sigma_i)}{\sum_c \omega_c \mathcal{N} (x_c, \mu_c, \sigma_c)} 
\end{align}
where $\omega_k$ is the component weights of component $k$ and $\mathcal{N}(x_i, \mu_i, \sigma_i)$ gives the probability of $x_i$ under component $k$.
\smallskip \\ \indent
The \textit{auxillary function} defined by Equation \ref{eq:eStep2} can then be applied to the result, $\gamma_{Z_i = k}$, where $\theta^{(t-1)}$ is the parameter generated in the previous iteration and $\theta^{(t)}$ is the new parameter value. Using Jensen's inequality, it can be proven that this auxiliary function is the lower bound of the gain of the likelihood that is obtained by updating the parameter values, but this proof is excluded for brevity.
\begin{align}
    \label{eq:eStep2}
    Q(\theta^{(t)}, \theta^{(t-1)}) &= \mathbb{E}\left[ \log \probP(Z \; | \; \theta^{(t)}) \; | \; X, \theta^{(t-1)} \right] \\
    &= \sum^M_{k=1} \log \mathbb{L} (\theta_k \; | \; X, Z) \probP(Z_k | X, \theta^{(t-1)}) \\
    &= \sum^M_{k=1} \log \mathbb{L} (\theta_k \; | \; X, Z) \; \gamma_{Z_i = k}
\end{align}
where $\log \mathbb{L} (\theta_k \; | \; X, Z)$ is the log likelihood of a Gaussian component with updated parameters and $\probP(Z_k | X, \theta^{(t-1)})$ is the distribution of latent variables according to the current parameters.
\smallskip \\ \indent
After the auxiliary function has been generated, the M-step can begin. This means maximising the value of $Q$ to produce the optimal parameter value in Equation \ref{eq:mStep1}.
\begin{equation}
    \label{eq:mStep1}
    \theta^{(t+1)} = argmax_\theta \; Q(\theta^{(t)}, \theta^{(t-1)})
\end{equation}
where
\begin{equation}
    \label{eq:mStep2}
    Q(\theta^{(t)}, \theta^{(t+1)}) = \sum^M_{k=1} \sum^N_{i=1} \log \gamma_k \probP(Z_k \; | \; X_i, \theta^{(t-1)}) + \sum^M_{k=1} \sum^N_{i=1} \log \probP (x_i \; | \; \theta_k) \probP (Z_k \; | \; X_i, \theta^{(t-1)})
\end{equation}
\indent
From this, the optimal parameter values can be derived by differentiating Equation \ref{eq:mStep2} with respect to the means, covariances, and weights, and solving when equal to zero, in turn. The results of these calculations are given Equation \ref{eq:mStep3}, Equation \ref{eq:mStep4}, and Equation \ref{eq:mStep5}, respectively. In the equations, $N_k = \sum^N_{i=1} \gamma_{Z_i = k}$.
\begin{equation}
    \label{eq:mStep3}
    \hat{\mu}_k = \frac{\sum^N_{i=1} X_i \probP(Z_i = k \; | \; X_i, \theta^{(t-1)})}{\sum^N_{i=1} \probP(Z_i = k \; | \; X_i, \theta^{(t-1)})} = \frac{1}{N_k} \sum^N_{i=1} \gamma_{Z_i = k} \; X_i
\end{equation}
\begin{equation}
    \label{eq:mStep4}
    \hat{\sigma^2}_k = \frac{1}{N_k} \sum^N_{i=1} \gamma_{Z_i=k} (X_i - \mu_k)^2
\end{equation}
\begin{equation}
    \label{eq:mStep5}
    \hat{\omega}_k = \frac{N_k}{N}
\end{equation}


\setlength{\leftskip}{0cm}




\section{Summary}
\setlength{\leftskip}{0.25cm}
\indent \indent
Overall, the project's implementation stage began with a foundational implementation of a client-server stack, followed by the integration of Microsoft's SEAL library to allow HE data transmission, satisfying the first core success criterion. After this, an investigation into the optimisation of the network stack involving the implementation of a \textit{seam carving} algorithm and \textit{graph representations} of images to reduce the size of videos and \textit{parallelisation} to increase the transmission rate between client and server.
\smallskip \\ \indent
The following implementation stage required inference algorithms to be converted to the HE domain to satisfy the second core success criterion. As part of this, several modifications had to be made to the algorithms, and investigations had to be conducted into implementing more HE Boolean circuits. 
\smallskip \\ \indent
Finally, a bespoke HE scheme was implemented from first principles following the CKKS scheme initially integrated. This allowed further understanding and an investigation into specialising the implementation as an opportunity for optimisation.


\setlength{\leftskip}{0cm}
